{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bEnc0p5U6mgdRMQ3tfvIKshRHyCzNRA8",
      "authorship_tag": "ABX9TyNzqvPZekChL4PPvQ1mtS5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c75dafca011643c5b29c6c3ac07fae72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08dcd5750f46489886a6b6f6833cb19e",
              "IPY_MODEL_7f768c2f8faf4cb5bb46ee7974a4baa0",
              "IPY_MODEL_c600d84cf3d44ba098bd6739026086a6"
            ],
            "layout": "IPY_MODEL_f27bd41bb60b4249bda7f6a1e1f12263"
          }
        },
        "08dcd5750f46489886a6b6f6833cb19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d68e9c26b934c559f201d66bd4567ad",
            "placeholder": "​",
            "style": "IPY_MODEL_f67da3a9dca94e1d8ee96640cbec3ee9",
            "value": "vocab.json: 100%"
          }
        },
        "7f768c2f8faf4cb5bb46ee7974a4baa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f5f52e429146e1a112fb854b32540c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ad8b373b7154be394ceb3495ce8a1a4",
            "value": 1042301
          }
        },
        "c600d84cf3d44ba098bd6739026086a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e9a62a12ebe48ff9b29570535398a64",
            "placeholder": "​",
            "style": "IPY_MODEL_5bf218abcc884aee9fc07f83dacdded5",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 7.57MB/s]"
          }
        },
        "f27bd41bb60b4249bda7f6a1e1f12263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d68e9c26b934c559f201d66bd4567ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67da3a9dca94e1d8ee96640cbec3ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f5f52e429146e1a112fb854b32540c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad8b373b7154be394ceb3495ce8a1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e9a62a12ebe48ff9b29570535398a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf218abcc884aee9fc07f83dacdded5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56462c9e9ecf4991a2564deb0874c4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_868f4bfa58a54527a16fd0dcd7d7cdf3",
              "IPY_MODEL_ae870cfa1a0c4aa8bf77d4c221dab8d1",
              "IPY_MODEL_c390c561b37a4a0eb71db4aa1101acd6"
            ],
            "layout": "IPY_MODEL_1b2b857a35214b79a5374ed1ce205b09"
          }
        },
        "868f4bfa58a54527a16fd0dcd7d7cdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a0437a53464c4bb146d5ceb3e7c566",
            "placeholder": "​",
            "style": "IPY_MODEL_f1382da90b2e4c2d97299c271de2bd3a",
            "value": "merges.txt: 100%"
          }
        },
        "ae870cfa1a0c4aa8bf77d4c221dab8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28094f5d5c2c46e98a7a9a58ed46629c",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e333023de91440281208d984d76500a",
            "value": 456318
          }
        },
        "c390c561b37a4a0eb71db4aa1101acd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_023ef609da014e2f83d6204705a3510b",
            "placeholder": "​",
            "style": "IPY_MODEL_19dfbe6dd2204511a7bfb00201790de6",
            "value": " 456k/456k [00:00&lt;00:00, 4.75MB/s]"
          }
        },
        "1b2b857a35214b79a5374ed1ce205b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a0437a53464c4bb146d5ceb3e7c566": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1382da90b2e4c2d97299c271de2bd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28094f5d5c2c46e98a7a9a58ed46629c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e333023de91440281208d984d76500a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "023ef609da014e2f83d6204705a3510b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19dfbe6dd2204511a7bfb00201790de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f50307f86ec4960bde10e7676453b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40ebbe38ba5e4e68a6e67cc69e00fa2d",
              "IPY_MODEL_9c4f7c63024a49caac4fdba873f3ae2f",
              "IPY_MODEL_16910e0878ff4520bfed807f9e720a57"
            ],
            "layout": "IPY_MODEL_4f3af8dcdb304fa98803948fe4087eeb"
          }
        },
        "40ebbe38ba5e4e68a6e67cc69e00fa2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4803cab9e034c32b8a4d78f140f8cd3",
            "placeholder": "​",
            "style": "IPY_MODEL_cd327c9b3b4e42f9873b3a7257d3771e",
            "value": "tokenizer.json: 100%"
          }
        },
        "9c4f7c63024a49caac4fdba873f3ae2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ceefa6ceb664c8f9e7df2938f451373",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb1d5d4071544ee29c1ec6faf015e1c7",
            "value": 1355256
          }
        },
        "16910e0878ff4520bfed807f9e720a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62df39aa8bf049bfb185a08b0730a928",
            "placeholder": "​",
            "style": "IPY_MODEL_73d7ea6dcbc441b6a670ec698d31ce2e",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 12.7MB/s]"
          }
        },
        "4f3af8dcdb304fa98803948fe4087eeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4803cab9e034c32b8a4d78f140f8cd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd327c9b3b4e42f9873b3a7257d3771e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ceefa6ceb664c8f9e7df2938f451373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb1d5d4071544ee29c1ec6faf015e1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62df39aa8bf049bfb185a08b0730a928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d7ea6dcbc441b6a670ec698d31ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4e2c295b0584de4b9c0cf22e78dbfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72422dfb9c9b4e98a8242957e50736e9",
              "IPY_MODEL_71322952397b483098e433edbed83fc0",
              "IPY_MODEL_6810017218394709831cb09f4b58c6a2"
            ],
            "layout": "IPY_MODEL_49c1a6d1b20740519987602ef2bfd6a2"
          }
        },
        "72422dfb9c9b4e98a8242957e50736e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcc6195840f4418ebda37017f86870a6",
            "placeholder": "​",
            "style": "IPY_MODEL_5e5de4b24b3d4123a34e699697e7d33f",
            "value": "config.json: 100%"
          }
        },
        "71322952397b483098e433edbed83fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad1b10655fa4b308eac2a96f15280e7",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a2e483403c84004ae4b2d8299d86bd7",
            "value": 665
          }
        },
        "6810017218394709831cb09f4b58c6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3f6a6b8010421b8fff8c66f170a1e9",
            "placeholder": "​",
            "style": "IPY_MODEL_6435e0152deb4c47962093156cab8d67",
            "value": " 665/665 [00:00&lt;00:00, 7.90kB/s]"
          }
        },
        "49c1a6d1b20740519987602ef2bfd6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcc6195840f4418ebda37017f86870a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5de4b24b3d4123a34e699697e7d33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fad1b10655fa4b308eac2a96f15280e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a2e483403c84004ae4b2d8299d86bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a3f6a6b8010421b8fff8c66f170a1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6435e0152deb4c47962093156cab8d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16377c0b9c484e05b6b893998ef511e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_305195e14a06488ba77ceb0716cd3624",
              "IPY_MODEL_128be0f2cf284a82a0ab97f9dfaf9f49",
              "IPY_MODEL_2265494df01b4747ab1fb94d8b0cde88"
            ],
            "layout": "IPY_MODEL_d2bfbcd4f16a49a8a59368999d35a7cf"
          }
        },
        "305195e14a06488ba77ceb0716cd3624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8575cb71cf954f7f84cf26e4bbdf9882",
            "placeholder": "​",
            "style": "IPY_MODEL_2528f547bdaa44d98afa47e68094e3d7",
            "value": "model.safetensors: 100%"
          }
        },
        "128be0f2cf284a82a0ab97f9dfaf9f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_964de14f342949118e9973a898001f68",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cde819ab5c3946ffa81c658ee93047e8",
            "value": 548105171
          }
        },
        "2265494df01b4747ab1fb94d8b0cde88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd925501b36c4264b6118d7c006d3427",
            "placeholder": "​",
            "style": "IPY_MODEL_0bd60762dd014646bb9a12956f7b9df6",
            "value": " 548M/548M [00:07&lt;00:00, 97.5MB/s]"
          }
        },
        "d2bfbcd4f16a49a8a59368999d35a7cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8575cb71cf954f7f84cf26e4bbdf9882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2528f547bdaa44d98afa47e68094e3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "964de14f342949118e9973a898001f68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cde819ab5c3946ffa81c658ee93047e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd925501b36c4264b6118d7c006d3427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd60762dd014646bb9a12956f7b9df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df46f080ed7f4a798608e6b1e36b4ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9677c6786844dd28e1b8fe3f24463f6",
              "IPY_MODEL_530f95af0f5d4a138543a5305a5ea5e6",
              "IPY_MODEL_bb71aa1d54874781858b3e6f4d36c265"
            ],
            "layout": "IPY_MODEL_7d1a188fee02452cb72260f63e98e114"
          }
        },
        "e9677c6786844dd28e1b8fe3f24463f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f49607046b9e4ec6acf6c18a99aad5f7",
            "placeholder": "​",
            "style": "IPY_MODEL_f6c49c27a5ef4f4fabf87c98f9e943b7",
            "value": "generation_config.json: 100%"
          }
        },
        "530f95af0f5d4a138543a5305a5ea5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a91df1c07a4654927f45c5180149d1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_598febdeec504a77b80cfc701a263d74",
            "value": 124
          }
        },
        "bb71aa1d54874781858b3e6f4d36c265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e070dcdf93843c8803d805cf38d2fba",
            "placeholder": "​",
            "style": "IPY_MODEL_f1b2c5e120494bd5b39b34fd2a960d90",
            "value": " 124/124 [00:00&lt;00:00, 5.01kB/s]"
          }
        },
        "7d1a188fee02452cb72260f63e98e114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49607046b9e4ec6acf6c18a99aad5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c49c27a5ef4f4fabf87c98f9e943b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01a91df1c07a4654927f45c5180149d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "598febdeec504a77b80cfc701a263d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e070dcdf93843c8803d805cf38d2fba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b2c5e120494bd5b39b34fd2a960d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david91107/predict_sklearn/blob/main/NLP_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is a work in progress"
      ],
      "metadata": {
        "id": "OaQU98RCj42U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generative AI"
      ],
      "metadata": {
        "id": "3Jf6ZVay1nfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a body of text to traing a language model.\n",
        "Text will come from a classic car forum powered by https://xenforo.com/.\n",
        "The intent is to leverage generative AI to create a \"virtual mechanic\" to help maintain older cars that have a dwindling set of experts available to turn to."
      ],
      "metadata": {
        "id": "Zie9UK8v8tKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Design Strategy\n"
      ],
      "metadata": {
        "id": "h1-7-AA1tR17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project will use the GPT-2 pre-trained model. This choice is due to the lower cost. The model will be supplemented with very specific data that is was not included in the training of GPT-2. An overview of the project is as follows:\n",
        "\n",
        "Here’s a basic outline on how to get started:\n",
        "\n",
        "1. Install Transformers Library\n",
        "Ensure you have the transformers library installed, which provides access to pre-trained models like GPT-2.\n",
        "\n",
        "3. Generate Responses\n",
        "You can input a prompt to the model and generate responses. Here’s an example:\n",
        "\n",
        "4. Fine-Tuning (Optional)\n",
        "If you have domain-specific data (like your car maintenance data), you can fine-tune GPT-2 on this data. This step is more advanced and requires setting up a training loop, handling data batching, and possibly using GPUs for efficient training.\n",
        "\n",
        "5. Deployment\n",
        "Once satisfied with the model's responses, you can integrate it into your application. For testing and development purposes, you can run the model on your local machine or use platforms like Google Colab for more computational power."
      ],
      "metadata": {
        "id": "lUpmK4rgtWNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Enviornment"
      ],
      "metadata": {
        "id": "mLQhNnniw35U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip3 install gensim==3.6.0\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCeKezlnzX3C",
        "outputId": "11eb2dab-7876-4554-83b3-068c6713ad8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0) (1.11.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY71JN6Sb-s7",
        "outputId": "a247c85c-55d3-4ae1-e29a-55b891220aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create corpus for a knowledge base"
      ],
      "metadata": {
        "id": "Cq_OvjOpsd2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of URLs to seach an parse\n",
        "\n",
        "url_list =[\n",
        "            #'https://e9coupe.com/forum/search/165678/'\n",
        "            #'https://e9coupe.com/forum/search/165679/'\n",
        "            #'https://e9coupe.com/forum/search/165680/'\n",
        "            #'https://e9coupe.com/forum/search/165682/'\n",
        "            #'https://e9coupe.com/forum/search/165703/'\n",
        "            #'https://e9coupe.com/forum/search/165704/'\n",
        "            #'https://e9coupe.com/forum/search/165705/'\n",
        "            'https://e9coupe.com/forum/search/169041/'\n",
        "            #'https://e9coupe.com/forum/threads/sway-bar-interchange.44955/'\n",
        "          ]\n",
        "\n",
        "# Create a df for the urls\n",
        "df_url = pd.DataFrame(url_list)\n",
        "\n",
        "column_names = ['url_list']\n",
        "\n",
        "# Assign the list to the DataFrame's columns\n",
        "df_url.columns = column_names\n",
        "\n",
        "# Export the df\n",
        "df_url.to_csv('e9coupe_df_url', index=False)"
      ],
      "metadata": {
        "id": "Ak2zFe-N8W3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def fetch_and_parse_thread(thread_url, thread_title, processed_posts):\n",
        "    numeric_thread_id = re.findall(r'\\d+', thread_url.split('/')[-2])[0] if re.findall(r'\\d+', thread_url.split('/')[-2]) else 'N/A'\n",
        "\n",
        "    response = requests.get(thread_url)\n",
        "    thread_data = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "        for article in articles:\n",
        "            post_id = article.get('id', 'N/A')\n",
        "            numeric_post_id = re.findall(r'\\d+', post_id)[0] if re.findall(r'\\d+', post_id) else 'N/A'\n",
        "\n",
        "            if numeric_post_id not in processed_posts:\n",
        "                processed_posts.add(numeric_post_id)\n",
        "                content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "                timestamp = article.find('time', class_='u-dt').get_text(strip=True) if article.find('time', class_='u-dt') else 'N/A'\n",
        "                post_number_element = article.find('ul', class_='message-attribution-opposite').find('li').find_next_sibling('li')\n",
        "                post_number = post_number_element.get_text(strip=True) if post_number_element else 'N/A'\n",
        "                post_number = post_number.lstrip('#') if post_number != 'N/A' else post_number\n",
        "\n",
        "                thread_data.append({\n",
        "                    'thread_id': numeric_thread_id,\n",
        "                    'thread_title': thread_title,  # Use the passed thread title\n",
        "                    'post_id': numeric_post_id,\n",
        "                    'timestamp': timestamp,\n",
        "                    'post_number': post_number,\n",
        "                    'post_raw': content\n",
        "                })\n",
        "\n",
        "    return thread_data\n",
        "\n",
        "def fetch_thread_urls_from_search(search_url):\n",
        "    response = requests.get(search_url)\n",
        "    thread_info = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        results = soup.find_all('div', class_='contentRow-main')\n",
        "        for result in results:\n",
        "            link = result.find('a', href=True)\n",
        "            title_element = result.find('h3', class_='contentRow-title')\n",
        "            if link and title_element:\n",
        "                full_url = 'https://e9coupe.com' + link['href']\n",
        "                thread_title = title_element.get_text(strip=True)\n",
        "                thread_info.append((full_url, thread_title))\n",
        "\n",
        "    return thread_info\n",
        "\n",
        "def process_all_threads(search_url):\n",
        "    thread_info = fetch_thread_urls_from_search(search_url)\n",
        "    processed_posts = set()\n",
        "    all_data = []\n",
        "\n",
        "    for url, title in thread_info:\n",
        "        thread_data = fetch_and_parse_thread(url, title, processed_posts)\n",
        "        all_data.extend(thread_data)\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# URL of the search results page\n",
        "search_url = \"https://e9coupe.com/forum/search/169041/\"\n",
        "\n",
        "# Process all threads and store in a DataFrame\n",
        "df = process_all_threads(search_url)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Export the df\n",
        "df.to_csv('e9coupe_df_raw.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYu29fbKknOX",
        "outputId": "2111739f-0669-49f4-fded-83cbcb7aa719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   thread_id                                       thread_title post_id  \\\n",
            "0         24                            E24 Restoration/rebuild  396138   \n",
            "1          9                  What did you do to your E9 today?  394424   \n",
            "2          9                  What did you do to your E9 today?  394427   \n",
            "3          9                  What did you do to your E9 today?  394440   \n",
            "4          9                  What did you do to your E9 today?  394459   \n",
            "..       ...                                                ...     ...   \n",
            "94     29745                           Still, could be worse...  238480   \n",
            "95     29024  Whereabouts of Ben Amen from La Jolla Independent  229858   \n",
            "96        28                       E28 headlight washer rebuild  204878   \n",
            "97        28                       E28 headlight washer rebuild  204890   \n",
            "98        28                       E28 headlight washer rebuild  204891   \n",
            "\n",
            "       timestamp post_number  \\\n",
            "0   Jan 20, 2024           1   \n",
            "1   Dec 22, 2023       3,861   \n",
            "2   Dec 22, 2023       3,862   \n",
            "3   Dec 22, 2023       3,863   \n",
            "4   Dec 22, 2023       3,864   \n",
            "..           ...         ...   \n",
            "94   Oct 4, 2018           5   \n",
            "95  Jun 26, 2018           1   \n",
            "96  Oct 31, 2017           1   \n",
            "97  Oct 31, 2017           2   \n",
            "98  Oct 31, 2017           3   \n",
            "\n",
            "                                             post_raw  \n",
            "0   I hadn't heard of someone powder coating an en...  \n",
            "1   Put a battery on, put new fresh fuel in, pumpe...  \n",
            "2   Scratch the uncounted day in row  stone ship o...  \n",
            "3   mr bump said:Put a battery on, put new fresh f...  \n",
            "4   By popular demand, HB Chris and others, took t...  \n",
            "..                                                ...  \n",
            "94  fooling around with someone elses wife and the...  \n",
            "95   Never mind. I found the thread with his details.  \n",
            "96  Hello,Trying to rebuild a set of e28 headlight...  \n",
            "97  I might have a spare washer motor unit laying ...  \n",
            "98     That would be fantastic. Sending you a PM now.  \n",
            "\n",
            "[99 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the text"
      ],
      "metadata": {
        "id": "5dZPaGkGsi5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "post_tokens = []\n",
        "\n",
        "for text in df['post_raw']:\n",
        "    post_token = word_tokenize(text.lower())  # Tokenize each post\n",
        "    post_token = [token for token in post_token if len(token) > 1]  # Filter out single-character tokens\n",
        "    post_tokens.append(post_token)\n",
        "\n",
        "df['post_tokens'] = post_tokens  # Add tokenized posts as a new column in the DataFrame\n",
        "\n",
        "# Stop words\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "post_filtered = []\n",
        "\n",
        "for tokens in df['post_tokens']:\n",
        "    filtered_tokens = [token for token in tokens if token not in mystopwords and token.isalpha()]\n",
        "    post_filtered.append(filtered_tokens)\n",
        "\n",
        "df['post_filtered'] = post_filtered  # Add filtered tokens as a new column\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemm_list = []\n",
        "\n",
        "for words in df['post_filtered']:\n",
        "    lemm_words = [lemmatizer.lemmatize(word) for word in words]  # Corrected list comprehension\n",
        "    lemm_list.append(lemm_words)\n",
        "\n",
        "df['post_lemm'] = lemm_list  # Add lemmatized words as a new column\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('e9coupe_df.csv', index=False)"
      ],
      "metadata": {
        "id": "ZgG8yFwVkwRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final df\n",
        "\n",
        "df['thread_id'] = df['thread_id'].fillna(0).astype(int)\n",
        "df['post_id'] = df['post_id'].fillna(0).astype(int)\n",
        "df['post_number'] = df['post_number'].str.replace(r'\\D', '', regex=True).astype(int)\n"
      ],
      "metadata": {
        "id": "rxd8SpwyQ8bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPh8hVZwNkPZ",
        "outputId": "35ec2333-fd62-4353-9af6-e6d57689a3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99 entries, 0 to 98\n",
            "Data columns (total 9 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   thread_id      99 non-null     int64 \n",
            " 1   thread_title   99 non-null     object\n",
            " 2   post_id        99 non-null     int64 \n",
            " 3   timestamp      99 non-null     object\n",
            " 4   post_number    99 non-null     int64 \n",
            " 5   post_raw       99 non-null     object\n",
            " 6   post_tokens    99 non-null     object\n",
            " 7   post_filtered  99 non-null     object\n",
            " 8   post_lemm      99 non-null     object\n",
            "dtypes: int64(3), object(6)\n",
            "memory usage: 7.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database Schema"
      ],
      "metadata": {
        "id": "ERTwY3LZeivj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Table 1: Issues\n",
        "<br>\n",
        "*   Key Should be from pandas\n",
        "*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "*   Issue Should be the thread title\n",
        "*   Short Description Should be from the thread post\n",
        "*   Keywords: Should be from the thread post\n",
        "\n",
        "Table 2: Solutions\n",
        "<br>\n",
        "*   Key Should be from pandas\n",
        "*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "*   Detailed Solution Should be from the thread posts\n",
        "*   Keywords Should be from the thread posts\n",
        "\n",
        "Table 3: Notes\n",
        "<br>\n",
        "*   Key Should be from pandas\n",
        "*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "*   Unstructured Note Content Should be from the thread posts"
      ],
      "metadata": {
        "id": "DQCLrAwhA6Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Table 1: Issues\n",
        "\n",
        "*   ID (Unique ID)\n",
        "*   Issue\n",
        "*   Short Description\n",
        "*   Keywords\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PqBySX4n2S7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Table 1 Issues\n",
        "\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_id\n",
        "#*   Issue Should be the thread title\n",
        "#*   Short Description Should be from the thread post\n",
        "#*   Keywords: Should be from the thread post\n",
        "\n",
        "\n",
        "df_table_1 = pd.DataFrame()\n",
        "\n",
        "# Specify the columns you want to copy\n",
        "columns_to_copy = ['thread_id','thread_title','post_number','post_raw']\n",
        "\n",
        "# Copy the specified columns\n",
        "df_table_1[columns_to_copy] = df[columns_to_copy].copy()"
      ],
      "metadata": {
        "id": "yk-nZAgAsvkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization/Short Description\n",
        "Very difficult to find a winning strategy here that can accomidate both long and short length text blocks.\n",
        "\n",
        "**Extractive Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Good with Raw Text: Extractive methods can work directly with raw, unstructured text, as they mainly focus on selecting key sentences or phrases without needing deep linguistic processing.\n",
        "Straightforward Implementation: These methods do not require complex preprocessing like tokenization or lemmatization, simplifying their implementation.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Limited Depth in Understanding: While they can handle raw text, they may not fully capture the nuanced meaning, especially when the text contains complex structures or unorthodox language use.\n",
        "Less Effective with Poorly Structured Text: In cases where the text is poorly structured or highly informal, extractive summarization might struggle to identify the main points effectively.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "**Abstractive Summarization** (like sshleifer/distilbart-cnn-12-6)\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Advanced Processing Capabilities: Abstractive models, especially those based on transformer architectures, are designed to handle and interpret raw text, capturing deeper linguistic and contextual nuances.\n",
        "Higher Tolerance for Unstructured Text: These models can manage unstructured or informal text by understanding and then rephrasing it in a more coherent and structured summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Dependence on Preprocessing for Optimal Performance: While they can process raw text, the quality of the output can be significantly improved with proper tokenization and lemmatization, especially for complex texts.\n",
        "Potential Overhead: Requires more computational resources to process and understand raw text, which might be more efficiently handled with some level of preprocessing.\n",
        "<br>\n",
        "<br>\n",
        "**Hybrid Summarization**\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Flexibility in Text Processing: Combining extractive and abstractive methods allows for handling both raw and preprocessed text, adapting to the text's structure and complexity.\n",
        "Balanced Approach: Can leverage the strengths of extractive methods in handling raw text for identifying key points, while using abstractive techniques for generating a coherent summary.\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Complex Preprocessing Requirements: The need to integrate both extractive and abstractive approaches may necessitate more sophisticated preprocessing strategies to optimize performance.\n",
        "Potential for Processing Inefficiencies: The combined approach might lead to redundancies or inefficiencies in processing, especially if the text is either too raw or overly preprocessed.\n",
        "<br>\n",
        "<br>\n",
        "Landed on a transformer model."
      ],
      "metadata": {
        "id": "r_YEpMzr3b5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summary"
      ],
      "metadata": {
        "id": "7jP2EWsDLobR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarizer (make sure to have transformers installed)\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    min_threshold = 0  # Threshold for text length\n",
        "    if len(text.split()) < min_threshold:\n",
        "        return text  # Return original for short texts\n",
        "    else:\n",
        "        try:\n",
        "            input_length = len(text.split())\n",
        "            max_length = max(10, int(input_length / 2))  # Example: half of the input length\n",
        "            min_length = max(5, int(max_length / 2))    # Ensuring min_length is not too small\n",
        "\n",
        "            return summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error during summarization: {e}\")\n",
        "            return text  # Return original text if summarization fails\n",
        "\n",
        "# Apply summarization conditionally\n",
        "#df_table_1['summary'] = df_table_1.apply(lambda x: summarize_text(x['post_raw']) if x['post_number'] == 1 else x['post_raw'], axis=1)\n",
        "df_table_1['summary'] = df_table_1.apply(lambda x: summarize_text(x['post_raw']) if x['post_number'] == 1 else '', axis=1)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df_table_1.to_csv('df_table_1.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJsG1Jw5JoCx",
        "outputId": "7eea68cd-e657-44cb-dc8d-fd6ab4736c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 10, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Your max_length is set to 10, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Your max_length is set to 10, but your input_length is only 9. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_count = df_table_1['summary'].nunique()\n",
        "print(unique_count)"
      ],
      "metadata": {
        "id": "31FRDpvrL3g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keywords"
      ],
      "metadata": {
        "id": "7-kzQjNm7dig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw text vs pre-processed\n",
        "Using Pre-Processed Data\n",
        "\n",
        "Fine-Tuning BERT or Similar Models\n",
        "Pros:\n",
        "\n",
        "Effective with Preprocessed and Raw Text: Can handle both preprocessed (for cleaner, more focused analysis) and raw text (to capture nuances and specific jargon).\n",
        "Good with Varied Text Lengths: Capable of managing both short and long texts, though long texts might need to be segmented due to token limits.\n",
        "Cons:\n",
        "\n",
        "Token Limitations for Long Texts: BERT has a token limit (512 tokens), necessitating additional processing for longer texts.\n",
        "Resource-Intensive: Requires significant computational resources, especially for processing and fine-tuning on large datasets.\n",
        "2. Using GPT-3 for Response Generation\n",
        "Pros:\n",
        "\n",
        "Adaptable to Both Text Types: Can generate responses based on both preprocessed and raw text, utilizing the context effectively.\n",
        "Handles Long Text Well: GPT-3's architecture allows it to handle longer sequences of text better than many other models, making it suitable for detailed responses.\n",
        "Cons:\n",
        "\n",
        "Context Dilution in Long Texts: Might lose context in very long text sequences, affecting response relevance.\n",
        "Operational Cost and Accessibility: Using GPT-3, particularly at scale, can be expensive and subject to usage constraints.\n",
        "3. Hybrid Approach (Combining Fine-Tuned BERT and GPT-3)\n",
        "Pros:\n",
        "\n",
        "Versatility with Text Types: Combines BERT's analytical strength (which can handle both preprocessed and raw text) with GPT-3’s generative capabilities.\n",
        "Effective with All Text Lengths: Can manage short texts efficiently and long texts through a combination of BERT's analytical precision and GPT-3's generative scope.\n",
        "Cons:\n",
        "\n",
        "Integration Complexity: Managing and integrating two systems, especially ensuring coherence between them, adds complexity.\n",
        "High Resource Requirement: Both models are resource-intensive, which could be a significant factor, particularly for processing large datasets.\n",
        "Additional Consideration: Preprocessed vs. Raw Text\n",
        "Preprocessed Text: While cleaner and more structured, may lose crucial context or technical terms, which can be pivotal in a specialized domain like automotive discussions.\n",
        "Raw Text: Retains all original information, including specific jargon and nuances, but is more challenging to process due to irregularities and noise."
      ],
      "metadata": {
        "id": "2qa85E_b9ea7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword extraction with BERT\n",
        "# Looks better\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract keywords using BERT\n",
        "def bert_extract_keywords(text, tokenizer, model, top_n=5):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Compute word importance by summing up the embeddings\n",
        "    word_importance = torch.sum(embeddings, dim=1)\n",
        "\n",
        "    # Get the indices of the top n important words\n",
        "    top_n_indices = word_importance.argsort(descending=True)[:top_n]\n",
        "\n",
        "    # Filter out indices that are out of range of input_ids\n",
        "    top_n_indices = [idx for idx in top_n_indices if idx < len(input_ids)]\n",
        "\n",
        "    # Decode the top n words\n",
        "    keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_n_indices]\n",
        "\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to each row in 'post_content'\n",
        "#df_table_1['keywords'] = df_table_1['post_raw'].apply(lambda x: bert_extract_keywords(x, tokenizer, model))\n",
        "df_table_1['keywords'] = df_table_1.apply(lambda x: bert_extract_keywords(x['post_raw'], tokenizer, model) if x['post_number'] == 1 else [], axis=1)\n",
        "\n",
        "\n",
        "# Display the DataFrame with extracted keywords\n",
        "print(df_table_1[['post_raw', 'keywords']])"
      ],
      "metadata": {
        "id": "PWGj_PDvEb40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalize table 1\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_id\n",
        "#*   Issue Should be the thread title\n",
        "#*   Short Description Should be from the thread post\n",
        "#*   Keywords: Should be from the thread post\n",
        "\n",
        "df_table_1.rename(columns={'thread_id': 'issue_id'}, inplace=True)\n",
        "df_table_1.rename(columns={'thread_title': 'issue'}, inplace=True)\n",
        "df_table_1.rename(columns={'summary': 'description'}, inplace=True)\n",
        "df_table_1.drop('post_raw', axis = 1, inplace=True)\n",
        "df_table_1.drop('post_number', axis = 1, inplace=True)\n",
        "\n",
        "df_issue = df_table_1.copy()\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df_issue.to_csv('df_issue.csv', index=False)"
      ],
      "metadata": {
        "id": "45eJ-TpoE3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Take a look\n",
        "\n",
        " df_issue.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmptCmNtbepW",
        "outputId": "775c787b-467c-497a-df86-8beb230a63e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99 entries, 0 to 98\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   issue_id     99 non-null     int64 \n",
            " 1   issue        99 non-null     object\n",
            " 2   description  99 non-null     object\n",
            " 3   keywords     99 non-null     object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Table 2: Solutions\n",
        "\n",
        "*   ID (Unique Note ID)\n",
        "*   Issue\n",
        "*   Issue ID (Foreign Key linking to ID in Issues table)\n",
        "*   Solution\n"
      ],
      "metadata": {
        "id": "u9T12lBX6m6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Table 2\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue Should be the thread title\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "#*   Detailed Solution Should be the concatinated post_raw per thread_id\n",
        "#*   Keywords Should be from be the concatinated post_raw per thread_id\n",
        "\n",
        "df_table_2 = pd.DataFrame()\n",
        "\n",
        "# Specify the columns you want to copy\n",
        "columns_to_copy = ['thread_id','thread_title']#, 'post_raw']\n",
        "\n",
        "# Copy the specified columns\n",
        "df_table_2[columns_to_copy] = df[columns_to_copy].copy()\n",
        "\n",
        "df_table_2 = df_table_2[['thread_id', 'thread_title']].drop_duplicates()"
      ],
      "metadata": {
        "id": "mg_0l-eCsvmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solutions and Keywords need to use concatenated post_raw values each post per thread\n",
        "def concatenate_posts_by_thread(df, group_col='thread_id', concat_col='post_raw'):\n",
        "\n",
        "    # Group by 'group_col' and concatenate 'concat_col' for each group\n",
        "    concatenated_df = df.groupby(group_col)[concat_col].apply(' '.join).reset_index()\n",
        "\n",
        "    # Rename the column to make it more descriptive\n",
        "    concatenated_column_name = f'concatenated_{concat_col}'\n",
        "    concatenated_df.rename(columns={concat_col: concatenated_column_name}, inplace=True)\n",
        "\n",
        "    return concatenated_df\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "concatenated_posts_df = concatenate_posts_by_thread(df)\n",
        "\n",
        "# Display the DataFrame with concatenated posts\n",
        "print(concatenated_posts_df)"
      ],
      "metadata": {
        "id": "RWuBio330RSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_posts_df.info()"
      ],
      "metadata": {
        "id": "VEXf2zrlAu6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalize table 2\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue Should be the thread title\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "#*   Detailed Solution Should be the concatinated post_raw per thread_id\n",
        "#*   Keywords Should be from be the concatinated post_raw per thread_id\n",
        "\n",
        "df_solutions = pd.merge(df_table_2, concatenated_posts_df, on='thread_id')\n",
        "\n",
        "df_solutions.rename(columns={'thread_title': 'issue'}, inplace=True)\n",
        "df_solutions.rename(columns={'thread_title': 'issue'}, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "gnnuiYY7Ch5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the DataFrame to a CSV file\n",
        "df_solutions.to_csv('df_solutions.csv', index=False)"
      ],
      "metadata": {
        "id": "WVAkf-8IGF1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look\n",
        "\n",
        "df_solutions.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68d_zgEBK_ef",
        "outputId": "0424fd92-8776-4726-f7d0-0289bdbfb88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 14 entries, 0 to 13\n",
            "Data columns (total 3 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   thread_id              14 non-null     int64 \n",
            " 1   issue                  14 non-null     object\n",
            " 2   concatenated_post_raw  14 non-null     object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 448.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Table 3: Notes\n",
        "\n",
        "#*   Key Should be from pandas\n",
        "#*   Issue ID (Foreign Key) Should be taken from the thread_key\n",
        "#*   Unstructured Content: Should be from the rawmthread posts\n",
        "\n",
        "df_table_3 = pd.DataFrame()\n",
        "\n",
        "# Specify the columns you want to copy\n",
        "columns_to_copy = ['thread_id','post_raw']\n",
        "\n",
        "# Copy the specified columns\n",
        "df_table_3[columns_to_copy] = df[columns_to_copy].copy()\n",
        "\n",
        "\n",
        "df_notes = df_table_3.copy()\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df_notes.to_csv('df_notes.csv', index=False)"
      ],
      "metadata": {
        "id": "KF92Opgbsvpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look\n",
        "\n",
        "df_notes.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rClL9YwdRCE",
        "outputId": "e360b4e5-6f83-44b7-dcfb-7847597f6a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 99 entries, 0 to 98\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   thread_id  99 non-null     int64 \n",
            " 1   post_raw   99 non-null     object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tables to Snowflake"
      ],
      "metadata": {
        "id": "xNen5FkiwXUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if I processed a complete auto repair guide to train on ??\n",
        "\n",
        "ChatGPT\n",
        "Training a keyword extraction model on a complete auto repair guide or similar domain-specific text is an excellent idea, especially if your goal is to improve the relevance and accuracy of keyword extraction for automotive discussions. Here's how you can approach this:\n",
        "\n",
        "Domain-Specific Corpus: Obtain a corpus of domain-specific texts, such as auto repair manuals, guides, or technical documents. This corpus should contain text that is closely related to the automotive domain and covers various aspects of auto repair, maintenance, and troubleshooting.\n",
        "\n",
        "Preprocessing: Preprocess the text data, similar to what you did with your forum data. This includes tokenization, stop word removal, lemmatization, and any other necessary cleaning steps.\n",
        "\n",
        "Keyword Labeling: Manually label or annotate the relevant keywords in this domain-specific text. You can do this by identifying words or phrases that are highly relevant to specific topics, parts, or procedures in the auto repair domain. This labeled data will serve as your training dataset.\n",
        "\n",
        "Training a Keyword Extraction Model: Utilize a machine learning or NLP model to train on this labeled dataset. You can explore different models, including traditional machine learning models (e.g., SVM, Random Forest) or deep learning models (e.g., BERT with fine-tuning). The model's objective should be to predict or extract keywords based on the labeled examples.\n",
        "\n",
        "Evaluation: After training the model, evaluate its performance using appropriate metrics. You can use metrics such as precision, recall, and F1-score to assess how well the model identifies keywords in the automotive domain.\n",
        "\n",
        "Fine-Tuning: Depending on the evaluation results, you may need to fine-tune the model to improve its keyword extraction accuracy. This can involve adjusting hyperparameters, increasing the amount of training data, or refining the labeling process.\n",
        "\n",
        "Integration: Once you have a well-trained keyword extraction model, you can integrate it into your project. You can use this model to extract keywords from the automotive forum discussions, improving the relevance and specificity of the extracted keywords.\n",
        "\n",
        "Continuous Learning: Keep in mind that language and terminology in the automotive field can evolve over time. It's essential to periodically update and retrain your keyword extraction model to stay current with the latest industry terminology and trends.\n",
        "\n",
        "By training a model on a domain-specific corpus, you can leverage the specialized knowledge contained in auto repair guides to enhance the accuracy and context-awareness of your keyword extraction process. This approach should help you generate more relevant and meaningful keywords for automotive discussions, ultimately improving the quality of your analysis and insights."
      ],
      "metadata": {
        "id": "4A1LTCEY1fCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model creation and training"
      ],
      "metadata": {
        "id": "5ricCI5guMw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "c75dafca011643c5b29c6c3ac07fae72",
            "08dcd5750f46489886a6b6f6833cb19e",
            "7f768c2f8faf4cb5bb46ee7974a4baa0",
            "c600d84cf3d44ba098bd6739026086a6",
            "f27bd41bb60b4249bda7f6a1e1f12263",
            "3d68e9c26b934c559f201d66bd4567ad",
            "f67da3a9dca94e1d8ee96640cbec3ee9",
            "88f5f52e429146e1a112fb854b32540c",
            "0ad8b373b7154be394ceb3495ce8a1a4",
            "9e9a62a12ebe48ff9b29570535398a64",
            "5bf218abcc884aee9fc07f83dacdded5",
            "56462c9e9ecf4991a2564deb0874c4f1",
            "868f4bfa58a54527a16fd0dcd7d7cdf3",
            "ae870cfa1a0c4aa8bf77d4c221dab8d1",
            "c390c561b37a4a0eb71db4aa1101acd6",
            "1b2b857a35214b79a5374ed1ce205b09",
            "03a0437a53464c4bb146d5ceb3e7c566",
            "f1382da90b2e4c2d97299c271de2bd3a",
            "28094f5d5c2c46e98a7a9a58ed46629c",
            "8e333023de91440281208d984d76500a",
            "023ef609da014e2f83d6204705a3510b",
            "19dfbe6dd2204511a7bfb00201790de6",
            "6f50307f86ec4960bde10e7676453b57",
            "40ebbe38ba5e4e68a6e67cc69e00fa2d",
            "9c4f7c63024a49caac4fdba873f3ae2f",
            "16910e0878ff4520bfed807f9e720a57",
            "4f3af8dcdb304fa98803948fe4087eeb",
            "c4803cab9e034c32b8a4d78f140f8cd3",
            "cd327c9b3b4e42f9873b3a7257d3771e",
            "0ceefa6ceb664c8f9e7df2938f451373",
            "fb1d5d4071544ee29c1ec6faf015e1c7",
            "62df39aa8bf049bfb185a08b0730a928",
            "73d7ea6dcbc441b6a670ec698d31ce2e",
            "e4e2c295b0584de4b9c0cf22e78dbfde",
            "72422dfb9c9b4e98a8242957e50736e9",
            "71322952397b483098e433edbed83fc0",
            "6810017218394709831cb09f4b58c6a2",
            "49c1a6d1b20740519987602ef2bfd6a2",
            "dcc6195840f4418ebda37017f86870a6",
            "5e5de4b24b3d4123a34e699697e7d33f",
            "fad1b10655fa4b308eac2a96f15280e7",
            "6a2e483403c84004ae4b2d8299d86bd7",
            "4a3f6a6b8010421b8fff8c66f170a1e9",
            "6435e0152deb4c47962093156cab8d67",
            "16377c0b9c484e05b6b893998ef511e1",
            "305195e14a06488ba77ceb0716cd3624",
            "128be0f2cf284a82a0ab97f9dfaf9f49",
            "2265494df01b4747ab1fb94d8b0cde88",
            "d2bfbcd4f16a49a8a59368999d35a7cf",
            "8575cb71cf954f7f84cf26e4bbdf9882",
            "2528f547bdaa44d98afa47e68094e3d7",
            "964de14f342949118e9973a898001f68",
            "cde819ab5c3946ffa81c658ee93047e8",
            "fd925501b36c4264b6118d7c006d3427",
            "0bd60762dd014646bb9a12956f7b9df6",
            "df46f080ed7f4a798608e6b1e36b4ef7",
            "e9677c6786844dd28e1b8fe3f24463f6",
            "530f95af0f5d4a138543a5305a5ea5e6",
            "bb71aa1d54874781858b3e6f4d36c265",
            "7d1a188fee02452cb72260f63e98e114",
            "f49607046b9e4ec6acf6c18a99aad5f7",
            "f6c49c27a5ef4f4fabf87c98f9e943b7",
            "01a91df1c07a4654927f45c5180149d1",
            "598febdeec504a77b80cfc701a263d74",
            "4e070dcdf93843c8803d805cf38d2fba",
            "f1b2c5e120494bd5b39b34fd2a960d90"
          ]
        },
        "id": "8lgZIzf3tLsO",
        "outputId": "82e899d0-8539-47ba-ac53-45d3b9620588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c75dafca011643c5b29c6c3ac07fae72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56462c9e9ecf4991a2564deb0874c4f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f50307f86ec4960bde10e7676453b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4e2c295b0584de4b9c0cf22e78dbfde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16377c0b9c484e05b6b893998ef511e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df46f080ed7f4a798608e6b1e36b4ef7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set tokenizer padding token with 'left' padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Define your prompt for generating a pizza recipe\n",
        "prompt = \"How do I replace the water pump on a 1973 BMW 3.0cs ?\"\n",
        "\n",
        "# Encode the prompt\n",
        "inputs = tokenizer.encode(prompt, return_tensors='pt', padding='max_length', max_length=100, truncation=True)\n",
        "\n",
        "# Generate text using the model\n",
        "outputs = model.generate(inputs, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# Decode and print the generated recipe\n",
        "generated_recipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_recipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URKZfwmIuVlw",
        "outputId": "56122da8-b59b-4142-f4f8-dccfce289da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How do I replace the water pump on a 1973 BMW 3.0cs?\n",
            "\n",
            "The water pump is a small, rectangular piece of metal that is attached to the front of the car. It is attached to the front of the car by a small, metal rod. The rod is attached to the front of the car by a small, metal rod. The rod is attached to the front of the car by a small, metal rod. The rod is attached to the front of the car by a small, metal rod. The rod is attached to the front of the car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parking Lot"
      ],
      "metadata": {
        "id": "uavr2PbRgYpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to extract keywords using BERT\n",
        "def bert_extract_keywords(text, tokenizer, model, top_n=5):\n",
        "    # Tokenize and encode the text\n",
        "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    # Get the embeddings from the last hidden layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "    # Compute word importance by summing up the embeddings\n",
        "    word_importance = torch.sum(embeddings, dim=1)\n",
        "\n",
        "    # Get the indices of the top n important words\n",
        "    top_n_indices = word_importance.argsort(descending=True)[:top_n]\n",
        "\n",
        "    # Filter out indices that are out of range of input_ids\n",
        "    top_n_indices = [idx for idx in top_n_indices if idx < len(input_ids)]\n",
        "\n",
        "    # Decode the top n words\n",
        "    keywords = [tokenizer.decode([input_ids[idx]]) for idx in top_n_indices]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to each row\n",
        "concatenated_posts_df['keywords'] = concatenated_posts_df['concatenated_post_raw'].apply(lambda x: bert_extract_keywords(x, tokenizer, model))\n"
      ],
      "metadata": {
        "id": "s10KkA39UOpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key word extraction with Tfid\n",
        "# Results are fast but terrible\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "\n",
        "# Fit the vectorizer to the entire dataset in df['post_content']\n",
        "vectorizer.fit(df['post_raw'])\n",
        "\n",
        "# Function to extract top N keywords for a given text entry\n",
        "def extract_keywords(text, vectorizer, top_n=5):\n",
        "    # Transform the text to tf-idf vector\n",
        "    tfidf_vector = vectorizer.transform([text])\n",
        "\n",
        "    # Retrieve the scores of the tf-idf vector\n",
        "    scores = tfidf_vector.toarray().flatten()\n",
        "\n",
        "    # Get the indices of the top n scores\n",
        "    top_n_indices = scores.argsort()[-top_n:][::-1]\n",
        "\n",
        "    # Get the feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Map indices to feature names (keywords)\n",
        "    keywords = [feature_names[i] for i in top_n_indices]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to each row in 'post_content'\n",
        "df_table_1['keywords'] = df_table_1['post_raw'].apply(lambda x: extract_keywords(x, vectorizer))\n",
        "\n",
        "# Display the DataFrame with extracted keywords\n",
        "print(df_table_1[['post_raw', 'keywords']])"
      ],
      "metadata": {
        "id": "5dtrkUKCCoB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keyword extraction with Spacy\n",
        "# Results are good but not great\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract keywords using spaCy\n",
        "def spacy_extract_keywords(text):\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract entities and noun chunks as keywords\n",
        "    # You can adjust this to suit your needs (e.g., include or exclude certain POS tags or entities)\n",
        "    keywords = [chunk.text for chunk in doc.noun_chunks] + [ent.text for ent in doc.ents]\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# Apply the function to each row in 'post_content'\n",
        "df['keywords'] = df['post_raw'].apply(spacy_extract_keywords)\n",
        "\n",
        "# Display the DataFrame with extracted keywords\n",
        "print(df[['post_raw', 'keywords']])"
      ],
      "metadata": {
        "id": "rBC8ZUgbEF7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization/Short Description\n",
        "\n",
        "# Hugging face token\n",
        "#XXXXXXXXXXXXXX\n",
        "\n",
        "# Initialize the summarizer (make sure to have transformers installed)\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    min_threshold = 50  # Threshold for text length\n",
        "    if len(text.split()) < min_threshold:\n",
        "        return text  # Return original for short texts\n",
        "    else:\n",
        "        try:\n",
        "            # Dynamically adjust max_length based on the input length\n",
        "            input_length = len(text.split())\n",
        "            max_length = max(30, int(input_length / 2))  # Example: half of the input length\n",
        "            min_length = max(10, int(max_length / 2))    # Ensuring min_length is not too small\n",
        "\n",
        "            return summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Error during summarization: {e}\")\n",
        "            return text  # Return original text if summarization fails\n",
        "\n",
        "# Assuming df_info is your DataFrame and 'post_content' is the column with texts\n",
        "summarized_content = [summarize_text(text) for text in df['post_raw']]\n",
        "\n",
        "# Add the summarized content as a new column\n",
        "df_table_1['summary'] = summarized_content\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df_table_1.to_csv('df_table_1.csv', index=False)"
      ],
      "metadata": {
        "id": "DwxFGpvJ3YBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dec 27 Version\n",
        "# Initialize lists\n",
        "thread_keys = []\n",
        "bread_crumbs = []\n",
        "titles = []\n",
        "post_keys = []\n",
        "post_contents = []\n",
        "post_authors = []\n",
        "\n",
        "# Assuming df_url['url_list'] is already defined and contains the URLs to scrape\n",
        "for url in df_url['url_list']:\n",
        "    page = requests.get(url)  # Send an HTTP GET request to the current URL\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    results = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "    for result in results:\n",
        "        title_element = result.find('h3', class_='contentRow-title')\n",
        "        if title_element and title_element.find('a'):\n",
        "            url_tail = title_element.find('a')['href']\n",
        "            detailed_url = 'https://e9coupe.com' + url_tail\n",
        "\n",
        "            page_detail = requests.get(detailed_url)  # Request for detailed page\n",
        "            soup_detail = BeautifulSoup(page_detail.content, \"html.parser\")\n",
        "\n",
        "            # Extract each breadcrumb item\n",
        "            breadcrumb_items = []\n",
        "            breadcrumbs = soup_detail.find('ul', class_='p-breadcrumbs')\n",
        "            if breadcrumbs:\n",
        "                for li in breadcrumbs.find_all('li', itemprop='itemListElement'):\n",
        "                    name = li.find('span', itemprop='name')\n",
        "                    if name:\n",
        "                        breadcrumb_items.append(name.get_text())\n",
        "\n",
        "            # Find thread key\n",
        "            html_tag = soup_detail.find('html')\n",
        "            if html_tag and html_tag.has_attr('data-content-key'):\n",
        "                thread_id = html_tag['data-content-key']\n",
        "                match = re.search(r'\\d+', thread_id)\n",
        "                thread_key = match.group() if match else 'none'\n",
        "            else:\n",
        "                thread_key = 'none'\n",
        "\n",
        "            # Find title\n",
        "            title_tag = soup_detail.find('title')\n",
        "            if title_tag:\n",
        "                thread_title = title_tag.get_text().split('|')[0].strip()\n",
        "            else:\n",
        "                thread_title = 'Title not found'\n",
        "\n",
        "            # Parse soup into articles for this particular thread\n",
        "            post_articles = soup_detail.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "            for article in post_articles:\n",
        "                post = article['data-content']\n",
        "                post_id = post.split(\"-\")[1]\n",
        "                post_author = article['data-author']\n",
        "                post_content = article.find('div', class_='bbWrapper').get_text(strip=True)\n",
        "\n",
        "                # Append the current post data\n",
        "                post_keys.append(post_id)\n",
        "                post_contents.append(post_content)\n",
        "                post_authors.append(post_author)\n",
        "\n",
        "                # Append the current thread_key, title, and breadcrumbs for this post\n",
        "                thread_keys.append(thread_key)\n",
        "                titles.append(thread_title)\n",
        "                bread_crumbs.append(breadcrumb_items)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'thread_key': thread_keys,\n",
        "    'title': titles,\n",
        "    'bread_crumb': bread_crumbs,\n",
        "    'post_key': post_keys,\n",
        "    'author': post_authors,\n",
        "    'post_content': post_contents\n",
        "})\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "#df.to_csv('e9coupe_df.csv', index=False)"
      ],
      "metadata": {
        "id": "eI3BdTmCcb41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is already defined and contains a column 'post_content'\n",
        "\n",
        "# Text tokenization\n",
        "post_tokens = []\n",
        "\n",
        "for text in df['post_content']:\n",
        "    post_token = word_tokenize(text.lower())  # Tokenize each post\n",
        "    post_token = [token for token in post_token if len(token) > 1]  # Filter out single-character tokens\n",
        "    post_tokens.append(post_token)\n",
        "\n",
        "df['post_tokens'] = post_tokens  # Add tokenized posts as a new column in the DataFrame\n",
        "\n",
        "# Remove stop words\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "post_filtered = []\n",
        "\n",
        "for tokens in df['post_tokens']:\n",
        "    filtered_tokens = [token for token in tokens if token not in mystopwords and token.isalpha()]\n",
        "    post_filtered.append(filtered_tokens)\n",
        "\n",
        "df['post_filtered'] = post_filtered  # Add filtered tokens as a new column\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemm_list = []\n",
        "\n",
        "for words in df['post_filtered']:\n",
        "    lemm_words = [lemmatizer.lemmatize(word) for word in words]  # Corrected list comprehension\n",
        "    lemm_list.append(lemm_words)\n",
        "\n",
        "df['post_lemm'] = lemm_list  # Add lemmatized words as a new column\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "#df.to_csv('e9coupe_df.csv', index=False)\n"
      ],
      "metadata": {
        "id": "x4pXdHgBG1WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Broken Image links"
      ],
      "metadata": {
        "id": "IwswQkOL2XuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test Cases"
      ],
      "metadata": {
        "id": "iZEIkpbzi0B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post with no images\n",
        "# https://e9coupe.com/forum/threads/how-to-find-threads-started-by-x.23058/\n",
        "\n",
        "# Post with one e9 hosted image\n",
        "# https://e9coupe.com/forum/threads/ex-han-stuck-csl-batmobile-sells-for-171-661.6560/\n",
        "\n",
        "# Post with 19 e9 hosted image\n",
        "# https://e9coupe.com/forum/threads/1966-2000cs.44749/\n",
        "\n",
        "# Post with 38 mixed hosting imgaes\n",
        "# X hosted images\n",
        "# Y broken links\n",
        "# https://e9coupe.com/forum/threads/interior-and-down-town-la-photoshoot-occoupe-3-0csi-update.23139/"
      ],
      "metadata": {
        "id": "s8kKRXnZ7v7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Code"
      ],
      "metadata": {
        "id": "TUEooF1fi6bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step:1\n",
        "# Create list of URLs to seach an parse\n",
        "# Need a way to programmatically create this list\n",
        "# I think to automate this I would need to access the API.\n",
        "\n",
        "\n",
        "url_list =[\n",
        "            #'https://e9coupe.com/forum/search/165678/'\n",
        "            #'https://e9coupe.com/forum/search/165679/'\n",
        "            #'https://e9coupe.com/forum/search/165680/'\n",
        "            #'https://e9coupe.com/forum/search/165682/'\n",
        "            #'https://e9coupe.com/forum/search/165703/'\n",
        "            #'https://e9coupe.com/forum/search/165704/'\n",
        "            #'https://e9coupe.com/forum/search/165705/'\n",
        "            'https://e9coupe.com/forum/search/168221/'\n",
        "          ]\n",
        "\n",
        "# Create a df for the urls\n",
        "df_1 = pd.DataFrame(url_list)\n",
        "\n",
        "column_names = ['url_list']#, 'Column_2', 'Column_3', ...]  # Add as many names as there are columns\n",
        "\n",
        "# Assign the list to the DataFrame's columns\n",
        "df_1.columns = column_names\n",
        "\n",
        "\n",
        "# Export the df\n",
        "# df_1.to_csv('e9coupe_df_1', index=False)\n",
        "\n",
        "# print(df_1)"
      ],
      "metadata": {
        "id": "sEt0bYpIndaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works to parse a post\n",
        "\n",
        "\n",
        "# Set the base URL\n",
        "URL_base = 'https://e9coupe.com/forum/threads/jeremy-clarkson-reveals-the-car-he-regrets-selling-the-most.37009/?page=1'\n",
        "\n",
        "# Initialize an empty list to store data from all pages\n",
        "all_data = []\n",
        "\n",
        "# Create a DataFrame from the collected dataa\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Send an HTTP GET request to the base URL\n",
        "page = requests.get(URL_base)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup_base = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "# Find the script tag containing the JSON data\n",
        "script_tag = soup_base.find('script', type='application/ld+json')\n",
        "\n",
        "# Extract the JSON data from the script tag (assuming it always exists)\n",
        "json_data = json.loads(script_tag.string)\n",
        "\n",
        "# Get the number of posts from the JSON data\n",
        "posts = json_data['interactionStatistic']['userInteractionCount']\n",
        "\n",
        "# Calculate the number of pages (assuming 20 posts per page)\n",
        "pages = math.ceil(posts / 20)\n",
        "\n",
        "# Find OP and URL\n",
        "script_tag = soup_base.find('script', type='application/ld+json')\n",
        "json_data = json.loads(script_tag.string)\n",
        "\n",
        "headline = json_data['headline']\n",
        "published = json_data['datePublished']\n",
        "date_part = published.split(\"T\")[0]\n",
        "author = json_data['author']['name']\n",
        "link_to_post = json_data['mainEntityOfPage']['@id']\n",
        "posts = json_data['interactionStatistic']['userInteractionCount'] + 1\n",
        "\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# df.to_csv('e9coupe_test.csv', index=False)\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "for i in range(1, pages + 1):\n",
        "    URL = URL_base + 'page-' + str(i)\n",
        "    # Send an HTTP GET request to the current URL\n",
        "    page = requests.get(URL)\n",
        "\n",
        "    # Parse the HTML content of the current page\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    # Find all post articles with the specified class attribute\n",
        "    post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "    # Iterate through the posts on the current page\n",
        "    for article in post_articles:\n",
        "        post_id = article['data-content']\n",
        "        post_author = article['data-author']\n",
        "\n",
        "        # Find attachments within the current post\n",
        "        att_tags = article.find_all('a', class_='file-preview js-lbImage')\n",
        "        total_att_count = len(att_tags)\n",
        "\n",
        "        # Find all images within the current post and count them\n",
        "        img_tags = article.find_all('img', class_='bbImage')\n",
        "        total_image_count = sum(1 for tag in img_tags)\n",
        "\n",
        "        # Filter e9 hosted images and count them\n",
        "        e9_hosted_image_count = sum(1 for tag in img_tags if 'e9coupe.com' in tag.get('src'))\n",
        "\n",
        "        # Filter 3rd party hosted images and count them\n",
        "        third_party_image_count = sum(1 for tag in img_tags if 'forum/proxy.php' in tag.get('src'))\n",
        "\n",
        "        # Append the extracted values to the all_data list\n",
        "        all_data.append({\n",
        "            'post_id': post_id,\n",
        "            'post_author': post_author,\n",
        "            'all_images': total_image_count,\n",
        "            'hosted_images': e9_hosted_image_count,\n",
        "            'linked_images': third_party_image_count,\n",
        "            'attachments': total_att_count,\n",
        "        })\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Count the number of posts processed\n",
        "num_articles = len(df)\n",
        "\n",
        "# Print report header\n",
        "print(f\"Headline:        {headline}\")\n",
        "print(f\"Author:          {author}\")\n",
        "print(f\"URL:             {link_to_post}\")\n",
        "print(f\"Published:       {date_part}\")\n",
        "\n",
        "print(f\"Total posts:     {posts}\")\n",
        "print(f\"Processed posts: {num_articles}\")\n",
        "print()\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# df.to_csv('e9coupe_test.csv', index=False)"
      ],
      "metadata": {
        "id": "mJDB0awxD3UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the collected dataa\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Initialize lists\n",
        "breadcrumb_items = []\n",
        "thread_keys = []\n",
        "titles = []\n",
        "post_keys = []\n",
        "post_contents = []\n",
        "post_authors = []\n",
        "\n",
        "# Parse the search page into URLs\n",
        "for url in df_url['url_list']:\n",
        "  page = requests.get(url) # Send an HTTP GET request to the current URL\n",
        "\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  results = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "  for link in results:\n",
        "      title_element = link.find('h3', class_='contentRow-title')\n",
        "      if title_element and title_element.find('a'):\n",
        "          title = title_element.get_text(strip=True)\n",
        "          url_tail = title_element.find('a')['href']\n",
        "          url = 'https://e9coupe.com'+url_tail\n",
        "\n",
        "          page = requests.get(url) # Send an HTTP GET request to the current URL\n",
        "          soup = BeautifulSoup(page.content, \"html.parser\") # Parse the HTML content of the current page\n",
        "\n",
        "\n",
        "          # Find the breadcrumb list\n",
        "          breadcrumbs = soup.find('ul', class_='p-breadcrumbs')\n",
        "\n",
        "          # Extract each breadcrumb item\n",
        "          if breadcrumbs:\n",
        "              for li in breadcrumbs.find_all('li', itemprop='itemListElement'):\n",
        "                  name = li.find('span', itemprop='name')\n",
        "                  if name:\n",
        "                      breadcrumb_items.append(name.get_text())\n",
        "              bread_crumbs.append(breadcrumb_items) # Append to a list\n",
        "\n",
        "\n",
        "          # Find title\n",
        "          title_tag = soup.find('title')\n",
        "          if title_tag:\n",
        "              thread_title = title_tag.get_text().split('|')[0].strip()\n",
        "          else:\n",
        "              thread_title = 'Title not found'\n",
        "\n",
        "          titles.append(thread_title) # Append to a list\n",
        "\n",
        "\n",
        "\n",
        "          # Find  thread key\n",
        "          html_tag = soup.find('html') # Find the <html> tag\n",
        "\n",
        "          # Extract the value of the 'data-content-key' attribute\n",
        "          if html_tag and html_tag.has_attr('data-content-key'):\n",
        "              thread_id = html_tag['data-content-key']\n",
        "              # Use a regular expression to extract the number value\n",
        "              match = re.search(r'\\d+', thread_id)\n",
        "              if match:\n",
        "                thread_key = match.group()\n",
        "\n",
        "          else:\n",
        "              thread_key = 'none'\n",
        "\n",
        "\n",
        "          thread_keys.append(thread_key) # Append to a list\n",
        "\n",
        "\n",
        "          # Parse soup into articles\n",
        "          post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "          # Iterate through the articles to get the post information\n",
        "          for article in post_articles:\n",
        "              post_id = article['data-content']\n",
        "              post_author = article['data-author']\n",
        "              post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "\n",
        "              # Append to a list\n",
        "              post_keys.append(post_id)\n",
        "              post_contents.append(post_content)\n",
        "              post_authors.append(post_author)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'bread_crumb': [bread_crumbs[0]] * len(post_keys),\n",
        "    'thread_keys': [thread_keys[0]] * len(post_keys),\n",
        "    'title': [titles[0]] * len(post_keys),  # Repeat the title for each post\n",
        "    'post_key': post_keys,\n",
        "    'author': post_authors,\n",
        "    'post_content': post_contents\n",
        "})\n",
        "\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# df.to_csv('e9coupe_df.csv', index=False)"
      ],
      "metadata": {
        "id": "hx6VinO725L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujNezFI6Z6mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Parse soup into articles\n",
        "# There is a thread and post key in each article\n",
        "# <a href=\"/forum/threads/wtb-vgs-slanted-intake-manifold.37038/post-314616\" rel=\"nofollow\">\n",
        "\n",
        "pages = 3\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "post_urls = []\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "\n",
        "for value in df_url['url_list']:\n",
        "  for i in range(1, pages + 1):\n",
        "      URL = URL_base + 'page-' + str(i)\n",
        "      # Send an HTTP GET request to the current URL\n",
        "      page = requests.get(URL)\n",
        "\n",
        "      # Parse the HTML content of the current page\n",
        "      soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "      # Find all post articles in the source\n",
        "      post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "      # Iterate through the posts on the current page\n",
        "      for article in post_articles:\n",
        "          print(url)\n",
        "          post_id = article['data-content']\n",
        "          print(post_id)\n",
        "          post_author = article['data-author']\n",
        "          print(post_author)\n",
        "          post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "          print(post_content)\n",
        "          print()\n",
        "          #print(article)\n",
        "          print('____________________________________________________________')"
      ],
      "metadata": {
        "id": "EOfaIAuesIP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing various parsing logic\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Web URL\n",
        "Web_url = \"https://e9coupe.com/forum/threads/holiday-time-has-come.44824/\"\n",
        "\n",
        "# Get URL Content\n",
        "r = requests.get(Web_url)\n",
        "\n",
        "# Parse HTML Code\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "print(soup.prettify())\n",
        "\n",
        "\n",
        "# Here is the header where you can get the thread ID. Should be only once per page\n",
        "<html id=\"XF\" lang=\"en-US\" dir=\"LTR\"\n",
        "\tdata-app=\"public\"\n",
        "\tdata-template=\"thread_view\"\n",
        "\tdata-container-key=\"node-10\"\n",
        "\tdata-content-key=\"thread-44782\"\n",
        "\tdata-logged-in=\"true\"\n",
        "\tdata-cookie-prefix=\"xf_\"\n",
        "\tdata-csrf=\"1705990108,264f4a124e9a97ea9c31e95179e29795\"\n",
        "\tclass=\"has-no-js template-thread_view\"\n",
        "\t>\n",
        "\n",
        "\n",
        "\n",
        "# Thread level Metadata\n",
        "#headline\": \"Holiday Time Has Come.\",\n",
        "#    \"articleBody\": \"Hi Members.  It's the time of year in this stressful life we all seem to be in to take time to dwell on all the nice times we have had in our lifetime. Take time to love your family, your friends and your neighbors. it's time to realize how lucky...\",\n",
        "#    \"articleSection\": \"Off-Topic\",\n",
        "#    \"author\": {\n",
        "#        \"@type\": \"Person\",\n",
        "#        \"name\": \"Koopman\"\n",
        "\n",
        "# See ifthis is once per \"page\" to get the threadID\n",
        "#<option value='{\"search_type\":\"post\",\"c\":{\"thread\":44824}}'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# After extracting the threadID, get the articles for that threadID\n",
        "# I think this is the \"article\" delimiter\n",
        "#<div class=\"message-content js-messageContent\">#\n",
        "# You can get a post ID here:\n",
        "#<div class=\"message-content js-messageContent\">\n",
        "#<div class=\"message-userContent lbContainer js-lbContainer\" data-lb-caption-desc=\"Koopman · Dec 24, 2023 at 6:45 PM\" data-lb-id=\"post-394598\">\n",
        "\n",
        "\n",
        "# This would allow me to join the post to the thread, and then use the thread name for classification, I think"
      ],
      "metadata": {
        "id": "dCqAoRVe25Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correctly classifies posts\n",
        "# Just need to find the post_id to join it back to step1\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "pages = 1\n",
        "\n",
        "\n",
        "for value in df_url['url_list']:\n",
        "    for i in range(1, pages + 1):\n",
        "        url = value + '?page=' + str(i)\n",
        "        page = requests.get(url)\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "for result in soup.find_all('div', class_='contentRow-main'):\n",
        "    title_element = result.find('h3', class_='contentRow-title')\n",
        "    if title_element and title_element.find('a'):\n",
        "        title = title_element.get_text(strip=True)\n",
        "        url = title_element.find('a')['href']\n",
        "\n",
        "        # Extract the integer following the period (.) using regex\n",
        "        thread_match = re.search(r'\\.(\\d+)', url)\n",
        "        thread_id = thread_match.group(1) if thread_match else None\n",
        "\n",
        "        # Extract the integer following \"post-\" using regex\n",
        "        post_match = re.search(r'post-(\\d+)', url)\n",
        "        post_id = post_match.group(1) if post_match else None\n",
        "\n",
        "\n",
        "\n",
        "        # Check if it's a thread or a post\n",
        "        post_info = result.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "        if \"Thread\" in post_info:\n",
        "            type = \"Thread\"\n",
        "        elif \"Post\" in post_info:\n",
        "            type = \"Post\"\n",
        "        else:\n",
        "            type = \"Unknown\"\n",
        "\n",
        "        print(f\"Title: {title}, URL: {url}, Type: {type}\")\n",
        "        print(f\"ThreadID: {thread_id}, PostID: {post_id}\")\n",
        "\n",
        "        print('--------------------------------------------------')\n",
        "\n",
        "        # Append the information to a list\n",
        "        post_urls.append({'title': title, 'url': url, 'type': type})\n"
      ],
      "metadata": {
        "id": "2XF-4qoJgPjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse soup into articles\n",
        "\n",
        "# Set the base URL\n",
        "URL_base = 'https://e9coupe.com/forum/threads/wtb-vgs-slanted-intake-manifold.37038/?page=1'\n",
        "\n",
        "# Initialize an empty list to store data from all pages\n",
        "all_data = []\n",
        "\n",
        "# Create a DataFrame from the collected dataa\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Parse the HTML content\n",
        "soup_base = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('e9coupe_test.csv', index=False)\n",
        "\n",
        "# Set the maximum number of pages to search in each URL\n",
        "pages = 3\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "post_urls = []\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "\n",
        "for url in df_url['url_list']:\n",
        "  for i in range(1, pages + 1):\n",
        "      URL_full = url + 'page-' + str(i)\n",
        "      # Send an HTTP GET request to the current URL\n",
        "      page = requests.get(URL_full)\n",
        "\n",
        "\n",
        "      # Parse the HTML content of the current page\n",
        "      soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "      # Find all post articles with the specified class attribute\n",
        "      post_articles = soup.find_all(class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "      # Iterate through the posts on the current page\n",
        "      for article in post_articles:\n",
        "          post_id = article['data-content']\n",
        "          post_author = article['data-author']\n",
        "          #post_content = soup.find('div', class_ = \"bbWrapper\")\n",
        "          post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "          print(post_id,post_author)\n",
        "          print(post_content)\n",
        "          print('____________________________________________________________')\n"
      ],
      "metadata": {
        "id": "7hCA1TnZpNKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step:2 Find and classify posts as threads\n",
        "\n",
        "# Set the maximum number of pages to search in each URL\n",
        "pages = 1\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "post_urls = []\n",
        "\n",
        "for value in df_url['url_list']:\n",
        "  for i in range(1, pages + 1):\n",
        "      URL = URL_base + 'page-' + str(i)\n",
        "      # Send an HTTP GET request to the current URL\n",
        "      page = requests.get(URL)\n",
        "\n",
        "      # Parse the HTML content of the current page\n",
        "      soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "      # Parse Soup to find articles\n",
        "      post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "      # Parse Soup to find post class information\n",
        "      post_class = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "      # Iterate through the article and extract the postID, Author and content\n",
        "      for article in post_articles:\n",
        "          post_id = article['data-content']\n",
        "          post_author = article['data-author']\n",
        "          print(post_id)\n",
        "          print(post_author)\n",
        "          #post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "          #print(post_content)\n",
        "          print('_________________End_of_first_for_loop_____________')\n",
        "\n",
        "      print('Start of second for loop')\n",
        "      print(f\"Number of elements in post_class: {len(post_class)}\")\n",
        "      # Iterate through the article and classify the content\n",
        "      for post in post_class:\n",
        "      # Check if it's a thread or a post\n",
        "          post_info = post.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "          if \"Thread\" in post_info:\n",
        "              type = \"Thread\"\n",
        "          elif \"Post\" in post_info:\n",
        "              type = \"Post\"\n",
        "          else:\n",
        "              type = \"Unknown\"\n",
        "          print(type)\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame for the URLs\n",
        "#df_2 = pd.DataFrame(post_urls)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "#df_2.to_csv('e9coupe_df_2.csv', index=False)"
      ],
      "metadata": {
        "id": "hWosENysO5MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1. Initial parsing of the source\n",
        "pages = 3\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "post_urls = []\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "\n",
        "for value in df_url['url_list']:\n",
        "  for i in range(1, pages + 1):\n",
        "      URL = URL_base + 'page-' + str(i)\n",
        "      # Send an HTTP GET request to the current URL\n",
        "      page = requests.get(URL)\n",
        "\n",
        "      # Parse the HTML content of the current page\n",
        "      soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "      # Find all post articles in the source\n",
        "      post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "      # Iterate through the posts on the current page\n",
        "      for article in post_articles:\n",
        "          print(article)\n",
        "          #post_id = article['data-content']\n",
        "          #post_author = article['data-author']\n",
        "          #print(post_id)\n",
        "          #print(post_author)\n",
        "          #post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "          #print(post_content)\n",
        "          print('____________________________________________________________')\n",
        "\n"
      ],
      "metadata": {
        "id": "8e4EOzMsHaIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Assuming df_url['url_list'] and URL_base are defined correctly\n",
        "# Set the maximum number of pages to search in each URL\n",
        "pages = 3\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "for value in df_url['url_list']:\n",
        "    for i in range(1, pages + 1):\n",
        "        URL = URL_base + 'page-' + str(i)\n",
        "        page = requests.get(URL)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "        print(f\"Processing URL: {URL}\")  # Debugging line\n",
        "\n",
        "        # Find all post articles in the source\n",
        "        #articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "        #print(f\"Found {len(articles)} articles\")  # Debugging line\n",
        "\n",
        "        #for article in articles:\n",
        "        #    post_id = article.get('data-content', 'N/A')\n",
        "        #    post_author = article.get('data-author', 'N/A')\n",
        "        #    print(f\"Post ID: {post_id}, Author: {post_author}\")\n",
        "\n",
        "        # Classify the type of post\n",
        "        posts = soup.find_all('div', class_='contentRow-main')\n",
        "        print(f\"Found {len(posts)} posts to classify\")  # Debugging line\n",
        "\n",
        "        for post in posts:\n",
        "            title_element = post.find('h3', class_='contentRow-title')\n",
        "            if title_element and title_element.find('a'):\n",
        "                title = title_element.get_text(strip=True)\n",
        "                url = title_element.find('a')['href']\n",
        "                post_info = post.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "                if \"Thread\" in post_info:\n",
        "                    type = \"Thread\"\n",
        "                elif \"Post\" in post_info:\n",
        "                    type = \"Post\"\n",
        "                else:\n",
        "                    type = \"Unknown\"\n",
        "                print(f\"Title: {title}, URL: {url}, Type: {type}\")\n",
        "\n",
        "        print('End of page processing')\n",
        "        print('____________________________________________________________')"
      ],
      "metadata": {
        "id": "1axuFmglMtpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "for result in results:\n",
        "    title_element = result.find('h3', class_='contentRow-title')\n",
        "    if title_element and title_element.find('a'):\n",
        "        title = title_element.get_text(strip=True)\n",
        "        url = title_element.find('a')['href']\n",
        "\n",
        "        # Check if it's a thread or a post\n",
        "        post_info = result.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "        if \"Thread\" in post_info:\n",
        "            type = \"Thread\"\n",
        "        elif \"Post\" in post_info:\n",
        "            type = \"Post\"\n",
        "        else:\n",
        "            type = \"Unknown\"\n",
        "\n",
        "        print(f\"Title: {title}, URL: {url}, Type: {type}\")\n",
        "        print('--------------------------------------------------')\n",
        "\n",
        "        # Append the information to a list\n",
        "        post_urls.append({'title': title, 'url': url, 'type': type})"
      ],
      "metadata": {
        "id": "OS4ffCulZErr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step:1\n",
        "# Create list of URLs to seach an parse\n",
        "# Need a way to programmatically create this list\n",
        "# I think to automate this I would need to access the API.\n",
        "\n",
        "\n",
        "url_list =[\n",
        "            #'https://e9coupe.com/forum/search/165678/'\n",
        "            #'https://e9coupe.com/forum/search/165679/'\n",
        "            #'https://e9coupe.com/forum/search/165680/'\n",
        "            #'https://e9coupe.com/forum/search/165682/'\n",
        "            #'https://e9coupe.com/forum/search/165703/'\n",
        "            #'https://e9coupe.com/forum/search/165704/'\n",
        "            #'https://e9coupe.com/forum/search/165705/'\n",
        "            'https://e9coupe.com/forum/search/168221/'\n",
        "          ]\n",
        "\n",
        "# Create a df for the urls\n",
        "df_1 = pd.DataFrame(url_list)\n",
        "\n",
        "column_names = ['url_list']#, 'Column_2', 'Column_3', ...]  # Add as many names as there are columns\n",
        "\n",
        "# Assign the list to the DataFrame's columns\n",
        "df_1.columns = column_names\n",
        "\n",
        "\n",
        "# Export the df\n",
        "df_1.to_csv('e9coupe_df_1', index=False)\n",
        "\n",
        "print(df_1)\n",
        "\n",
        "\n",
        "\n",
        "# Step:2 Find threads\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Set the maximum number of pages to search in each URL\n",
        "pages = 3\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "thread_urls = []\n",
        "\n",
        "for value in df_1['url_list']:\n",
        "    for i in range(1, pages + 1):\n",
        "        url = value + '?page=' + str(i)\n",
        "        page = requests.get(url)\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        results = soup.find_all('div', class_='contentRow-main')\n",
        "\n",
        "        for result in results:\n",
        "            title_element = result.find('h3', class_='contentRow-title')\n",
        "            if title_element and title_element.find('a'):\n",
        "                title = title_element.get_text(strip=True)\n",
        "                url = title_element.find('a')['href']\n",
        "\n",
        "                # Check if it's a thread or a post\n",
        "                post_info = result.find('div', class_='contentRow-minor').get_text(strip=True)\n",
        "                if \"Thread\" in post_info:\n",
        "                    type = \"Thread\"\n",
        "\n",
        "                    print(f\"Title: {title}, URL: {url}, Type: {type}\")\n",
        "                    print('--------------------------------------------------')\n",
        "                # Append the information to a list\n",
        "                    url = 'https://e9coupe.com'+url\n",
        "                    thread_urls.append({'url': url})\n",
        "                    #thread_urls.append({'title': title, 'url': url, 'type': type})\n",
        "\n",
        "\n",
        "# Create a DataFrame for the URLs\n",
        "df_threads = pd.DataFrame(thread_urls)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# df_threads.to_csv('e9coupe_df_threads.csv', index=False)"
      ],
      "metadata": {
        "id": "UofAq5P5Zzc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step:3 Find posts\n",
        "\n",
        "\n",
        "# Set the base URL\n",
        "URL_base = 'https://e9coupe.com/forum/threads/wtb-vgs-slanted-intake-manifold.37038/?page=1'\n",
        "\n",
        "# Initialize an empty list to store data from all pages\n",
        "all_data = []\n",
        "\n",
        "# Create a DataFrame from the collected dataa\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Send an HTTP GET request to the base URL\n",
        "page = requests.get(URL_base)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup_base = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "# Find the script tag containing the JSON data\n",
        "script_tag = soup_base.find('script', type='application/ld+json')\n",
        "\n",
        "# Extract the JSON data from the script tag (assuming it always exists)\n",
        "json_data = json.loads(script_tag.string)\n",
        "\n",
        "# Get the number of posts from the JSON data\n",
        "posts = json_data['interactionStatistic']['userInteractionCount']\n",
        "\n",
        "# Calculate the number of pages (assuming 20 posts per page)\n",
        "pages = math.ceil(posts / 20)\n",
        "\n",
        "# Find OP and URL\n",
        "script_tag = soup_base.find('script', type='application/ld+json')\n",
        "json_data = json.loads(script_tag.string)\n",
        "\n",
        "headline = json_data['headline']\n",
        "published = json_data['datePublished']\n",
        "date_part = published.split(\"T\")[0]\n",
        "author = json_data['author']['name']\n",
        "link_to_post = json_data['mainEntityOfPage']['@id']\n",
        "posts = json_data['interactionStatistic']['userInteractionCount'] + 1\n",
        "\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "df.to_csv('e9coupe_test.csv', index=False)\n",
        "\n",
        "# Set the maximum number of pages to search in each URL\n",
        "pages = 3\n",
        "\n",
        "# Initialize an empty list to store URLs of all posts\n",
        "post_urls = []\n",
        "\n",
        "# Loop to increment and parse the URLs\n",
        "\n",
        "for value in df_1['url_list']:\n",
        "  for i in range(1, pages + 1):\n",
        "      URL = URL_base + 'page-' + str(i)\n",
        "      # Send an HTTP GET request to the current URL\n",
        "      page = requests.get(URL)\n",
        "\n",
        "      # Parse the HTML content of the current page\n",
        "      soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "      # Find all post articles with the specified class attribute\n",
        "      post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "\n",
        "      # Iterate through the posts on the current page\n",
        "      for article in post_articles:\n",
        "          post_id = article['data-content']\n",
        "          post_author = article['data-author']\n",
        "          #post_content = soup.find('div', class_ = \"bbWrapper\")\n",
        "          post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "          print(post_id,post_author)\n",
        "          print(post_content)\n",
        "          print('____________________________________________________________')\n"
      ],
      "metadata": {
        "id": "JKl2Zk7aZ6hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the body of the page into articles\n",
        "\n",
        "url = 'https://e9coupe.com/forum/threads/holiday-time-has-come.44824/'\n",
        "page = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the current page\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "# Find all post articles in the source\n",
        "post_articles = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')\n",
        "\n",
        "# Iterate through the posts on the current page\n",
        "for article in post_articles:\n",
        "    print(url)\n",
        "    post_id = article['data-content']\n",
        "    print(post_id)\n",
        "    post_author = article['data-author']\n",
        "    print(post_author)\n",
        "    post_content = article.find('div', class_ = \"bbWrapper\").get_text(strip=True)\n",
        "    print(post_content)\n",
        "    print()\n",
        "    #print(article)\n",
        "    print('____________________________________________________________')\n"
      ],
      "metadata": {
        "id": "comM5TPx25OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: 3\n",
        "# Find articles from each URL\n",
        "# Find images from each article\n",
        "# This might be easier if I had API access, but I guess HTML scraping is all about string searches\n",
        "\n",
        "div_found_flag = 0\n",
        "data_url_flag = 0\n",
        "data_src_flag = 0\n",
        "image_flag = 0\n",
        "article_post_meta =[]\n",
        "df_3 = pd.DataFrame(article_post_meta)\n",
        "\n",
        "for value in df_2['post_url']:\n",
        "  url = 'https://e9coupe.com' + value\n",
        "  page = requests.get(url)\n",
        "\n",
        "  # Create a soup value form the URL\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "  # Parse the soup content of the ULR into delimited articles\n",
        "  article_list = soup.find_all('article', class_='message message--post js-post js-inlineModContainer')#, attrs={'data-author'})\n",
        "\n",
        "  # Parse the article content of the soup into delimited image lines\n",
        "  for article in article_list:\n",
        "    div_found_flag = 0\n",
        "    data_src_flag = 0\n",
        "    data_src_image_string_flag = 0\n",
        "    image_url_raw = ''\n",
        "\n",
        "    img_divs = article.find_all('div', class_=\"bbImageWrapper js-lbImage\")\n",
        "    for div in img_divs:\n",
        "      if div:\n",
        "        div_found_flag = 1\n",
        "\n",
        "        if div.get('data-src'):\n",
        "          data_src_flag = 1\n",
        "\n",
        "        if 'image' in div.get('data-src'):\n",
        "              data_src_image_string_flag = 1\n",
        "\n",
        "    # Build the image URL\n",
        "      if data_src_image_string_flag == 1:\n",
        "        image_url_raw = div.get('data-src')\n",
        "        if 'e9coupe.com/forum/attachments/' not in image_url_raw:\n",
        "          image_url_encoded = image_url_raw.split('image=')[1].split('&hash=')[0]\n",
        "          image_url = urllib.parse.unquote(image_url_encoded)\n",
        "      else:\n",
        "        image_url = div.get('data-src')\n",
        "\n",
        "# Ideally the author value would be passed in\n",
        "# Some of these links are my replies, rather than post. This means I cannot address broken links--the OP would need to.\n",
        "# Im grabbing a nested div by mistake.\n",
        "\n",
        "    if div_found_flag == 1 and article['data-author'] == 'David' and 'attachments' not in image_url:\n",
        "      print('----------------------------------------')\n",
        "      print('----------------------------------------')\n",
        "      print('Topic url:  '               ,url)  # Print the URL for each article\n",
        "      #print('New article')\n",
        "      post_id = article['data-content']\n",
        "      print('Post ID.:'                   ,post_id)\n",
        "      author = article['data-author']\n",
        "      print('Post author:'                ,author)\n",
        "      #print('div_found_flag:            ',div_found_flag)\n",
        "      #print('data_src_flag:             ',data_src_flag)\n",
        "      #print('data_src_image_string_flag:',data_src_image_string_flag)\n",
        "      print(image_url)\n",
        "      # Append the extracted values to the df\n",
        "      article_post_meta.append({\n",
        "          'topic_url': url,\n",
        "          'image_url' : image_url,\n",
        "          'post_id' : article['data-content'],\n",
        "          'author' : article['data-author']\n",
        "      })\n",
        "\n",
        "# Create a DataFrame from the collected data\n",
        "df_3 = pd.DataFrame(article_post_meta)\n",
        "\n",
        "# Export the df\n",
        "# df_3.to_csv('e9coupe_df_3.csv', index=False)"
      ],
      "metadata": {
        "id": "ejVm_wei2hm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: 4\n",
        "# Create a report\n",
        "# I havent updated this with the changes I made to Steps 1-3 so it might not work.\n",
        "\n",
        "# Count the number of posts processed\n",
        "num_articles = len(df)\n",
        "\n",
        "\n",
        "# Print report header\n",
        "print(f\"Headline:        {headline}\")\n",
        "print(f\"Author:          {author}\")\n",
        "print(f\"URL:             {link_to_post}\")\n",
        "print(f\"Published:       {date_part}\")\n",
        "\n",
        "print(f\"Total posts:     {posts}\")\n",
        "print(f\"Processed posts: {num_articles}\")\n",
        "print()\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "# df_4.to_csv('e9coupe_df_4.csv', index=False)"
      ],
      "metadata": {
        "id": "ddjg30RRoxVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}