{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david91107/predict_sklearn/blob/main/predictive_modeling_with_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8bxhyrQmZb6"
      },
      "source": [
        "#Forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmPOAm2W_I9V"
      },
      "source": [
        "In this Jupyter notebook, I present an AI model designed to identify and quantify the key factors contributing to customer churn in a subscription-based service. This model demonstrates adeptness in leveraging advanced machine learning techniques and data analytics to solve real-world business problems. By meticulously experimenting with various machine learning algorithms, the developed model not only predicts churn with high accuracy but also provides insightful interpretations of the underlying causes. The results shown were implemented recently in a subscription based business and reduced churn by > 15%. This project demonstrates proficiency in Python programming, deep understanding of predictive modeling, and an ability to translate complex data insights into actionable business strategies that produce results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3bBVbNF7M3X"
      },
      "source": [
        "#Predictive Subscriber Cancellation Model for a Subscription Service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyCwjxuU9g6r"
      },
      "source": [
        "\n",
        "**Notebook Chapters**\n",
        "\n",
        "Project Introduction\n",
        "\n",
        "Model Selection\n",
        "\n",
        "Enviornment Creation\n",
        "\n",
        "Data Ingestion\n",
        "\n",
        "Exploratory Data Analysis\n",
        "\n",
        "Model Comparison\n",
        "\n",
        "Model Interpretation and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzPjStWb_OiW"
      },
      "source": [
        "##Project Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWhFo6uasTPG"
      },
      "source": [
        "The project uses data from a subscription service to indentify factors and their importance in reducing customer service cancellations (aka, \"churn\"). Data has been carefully pre-processed to remove Personally Identifiable Information as well as information specific to the business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvk4En_jBIbv"
      },
      "source": [
        "##Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YemM0rxXrfuL"
      },
      "source": [
        "Mutiple models where consdiered for this project. Given the constraints of low compute costs, a small dataset, class imbalanced and the need to identify feature weights, the following models were used:\n",
        "\n",
        "*   Logistic Regression<br>\n",
        "Pros: With a small dataset and limited compute resources, logistic regression is a great starting point due to its simplicity and low computational cost. It also provides direct insight into feature importance.<br>\n",
        "Cons: Its linear nature might not capture complex relationships.<br>\n",
        "<br>\n",
        "*   Decision Trees<br>\n",
        "Pros: They are relatively simple to understand and can provide clear insights into which features are most important in predicting churn.<br>\n",
        "Cons: Prone to overfitting, especially with small datasets.<br>\n",
        "<br>\n",
        "*   Random Forest<br>\n",
        "Pros: Can handle imbalanced classes better than many models and provides feature importance. It's also less likely to overfit compared to a single decision tree.<br>\n",
        "Cons: More computationally intensive than logistic regression or a single decision<br>\n",
        "<br>\n",
        "*   Gradient Boosting Machines <br>\n",
        "Pros: Often more effective than random forests, especially with small datasets. Provides feature importance.<br>\n",
        "Cons: Can be computationally expensive and prone to overfitting.<br>\n",
        "<br>\n",
        "\n",
        "Results for all models are shown and compared below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g59qe_rsk-DJ"
      },
      "source": [
        "##Enviornment Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-rqyBKdYMol"
      },
      "outputs": [],
      "source": [
        "# Create python enviornment\n",
        "\n",
        "# Install libraries\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install scikit-learn\n",
        "!pip install pandas==1.5.3 # to be compatible with google-colab 1.0.0\n",
        "!pip install matplotlib\n",
        "!pip install shap\n",
        "\n",
        "# Install methods\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning) # Ignore FutureWarning\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve,roc_auc_score, accuracy_score,f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYnXMx8khjtI"
      },
      "source": [
        "##Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arUD5_dlBU8f"
      },
      "outputs": [],
      "source": [
        "# Load CSV data file into a dataframe\n",
        "\n",
        "df_pre = pd.read_csv(r'/content/drive/MyDrive/Data_sets/telco_subscription_data.csv')\n",
        "\n",
        "# Check data load\n",
        "\n",
        "df_pre.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXrgggFBVD7"
      },
      "outputs": [],
      "source": [
        "# Address null values\n",
        "\n",
        "# Look for Nulls\n",
        "\n",
        "df_pre.info()\n",
        "\n",
        "# Drop Nulls\n",
        "\n",
        "df_pre.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBTOWsq3xaIx"
      },
      "outputs": [],
      "source": [
        "# Identify continuous vs categorical data\n",
        "# based on count of unique values\n",
        "\n",
        "for column in df_pre.columns:\n",
        "    # Get the unique count of the current column\n",
        "    unique_count = df_pre[column].nunique()\n",
        "    print(f\"Unique values in '{column}': {unique_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykpXjPXngPpH"
      },
      "outputs": [],
      "source": [
        "# Some of the int values are for features that look categorical\n",
        "# They need to be string values that will be later flattened\n",
        "\n",
        "columns_to_cast = ['sales_team', 'sales_channel','sales_program','product_sku','product_cpe','customer_demo_geography']\n",
        "\n",
        "df_pre[columns_to_cast] = df_pre[columns_to_cast].astype(object)\n",
        "\n",
        "# Some of the float64 values are actually int values\n",
        "# that will need to be flattened\n",
        "\n",
        "# Define a list of column names to cast\n",
        "columns_to_cast = ['product_operations_kpi1']\n",
        "\n",
        "# Cast the columns\n",
        "df_pre[columns_to_cast] = df_pre[columns_to_cast].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1ht5aDeZaZq"
      },
      "source": [
        "##Explority Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWS_eJ7oUPU"
      },
      "outputs": [],
      "source": [
        "# Exploratory plot for the dependent/taget variable.\n",
        "\n",
        "df_pre.target.value_counts().plot(kind='bar', title='Dependent Variable/target')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_MO80i7yc_G"
      },
      "source": [
        "Data has inbalance for the dependant/target varabile. (boolean for the account status where 1 = churn, and 0 = active). Will need to address in the final dataset preparation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfmNal4c_-Y"
      },
      "outputs": [],
      "source": [
        "# Exploratory plots for features with continuous values\n",
        "\n",
        "feature_lst = ['customer_demo_age','product_operations_kpi1','product_operations_kpi2','product_usage_kpi1','product_usage_kpi2','product_usage_kpi3']\n",
        "\n",
        "df_eda = df_pre[feature_lst]\n",
        "\n",
        "# Create histograms for each feature in the dataframe\n",
        "axes = df_eda.hist(bins=50, figsize=(8, 8))\n",
        "\n",
        "# Iterate through axes and set font sizes\n",
        "for ax in axes.flatten():\n",
        "    ax.set_title(ax.get_title(), fontsize=10)\n",
        "    ax.set_xlabel(ax.get_xlabel(), fontsize=8)\n",
        "    ax.set_ylabel(ax.get_ylabel(), fontsize=8)\n",
        "    ax.tick_params(axis='x', labelsize=6)\n",
        "    ax.tick_params(axis='y', labelsize=6)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqdOa0jYOlAW"
      },
      "outputs": [],
      "source": [
        "# Pair plots\n",
        "sns.pairplot(df_pre)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7H-bFoXBVHv"
      },
      "outputs": [],
      "source": [
        "# Collinearity: Pearson\n",
        "\n",
        "df_num = df_pre.select_dtypes(include = ['float64', 'int64'])\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "corrMatrix = df_num.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-EK8EYlb6Jv"
      },
      "outputs": [],
      "source": [
        "# Collinearity: VIF\n",
        "\n",
        "df_num = df_pre.select_dtypes(include = ['float64', 'int64'])\n",
        "\n",
        "# Calculate VIF for each numeric feature\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df_num.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df_num.values, i) for i in range(df_num.shape[1])]\n",
        "\n",
        "# Print the VIF values to identify multicollinearity among numeric features\n",
        "print(\"VIF Values for Numeric Features:\")\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx_I4lt3ux0t"
      },
      "source": [
        "Features appear to be largely independet as shown by Perason and VIF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx0z-_k-1b1E"
      },
      "source": [
        "##Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVfjjnzFMgUn"
      },
      "outputs": [],
      "source": [
        "# Normalize continuous features\n",
        "columns_to_norm = ['product_usage_kpi1','product_usage_kpi2', 'product_usage_kpi3','product_operations_kpi1','product_operations_kpi2']\n",
        "\n",
        "for column in columns_to_norm:\n",
        "  df_pre[column] = df_pre[column] / df_pre[column].abs().max()\n",
        "\n",
        "# Bin age into generations\n",
        "# Assumption is that trends fall along generations more than specific ages\n",
        "# https://www.beresfordresearch.com/age-range-by-generation/\n",
        "\n",
        "df_pre['customer_demo_age_gen'] = df_pre['customer_demo_age'].apply(lambda age:\n",
        "    'gen_z' if age >= 12 and age <= 27 else\n",
        "    'millennial' if age >= 28 and age <= 43 else\n",
        "    'gen_x' if age >= 44 and age <= 59 else\n",
        "    'boomers' if age >= 60 and age <= 78 else\n",
        "    'post_war' if age >= 79 and age <= 96 else\n",
        "    'invalid' if age < 18 and age > 99 else\n",
        "    'other'\n",
        ")\n",
        "\n",
        "# Drop age\n",
        "df_pre.drop('customer_demo_age', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Flatten categorical features\n",
        "categorical_features = ['product_sku','sales_channel','sales_team','product_cpe','sales_program','customer_demo_age_gen','customer_demo_geography']\n",
        "\n",
        "# Perform one-hot encoding (flattening) of the specified columns\n",
        "df_pre = pd.get_dummies(df_pre, columns=categorical_features)\n",
        "\n",
        "# Convert boolean values to integers (1 and 0)\n",
        "df_pre = df_pre.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUQ7kiLJh6-g"
      },
      "source": [
        "###Feature Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZdXcM4hzkfZ"
      },
      "outputs": [],
      "source": [
        "# Initial feature counts\n",
        "\n",
        "unique_column_names = set(df_pre.columns)\n",
        "column_count = len(unique_column_names)\n",
        "print(f\"Total Features: {column_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-gRM3M67gR_"
      },
      "outputs": [],
      "source": [
        "# Perform ANOVA tests for each feature and reduce features\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "alpha = 0.05\n",
        "anova_results = []\n",
        "features_to_drop = []\n",
        "target_variable = 'target'\n",
        "\n",
        "for feature_column in df_pre.columns:\n",
        "    if feature_column != target_variable:\n",
        "        # Group data by target variable and perform ANOVA\n",
        "        grouped_data = [group[1] for group in df_pre.groupby(target_variable)[feature_column]]\n",
        "        f_statistic, p_value = f_oneway(*grouped_data)\n",
        "        if p_value >= alpha:\n",
        "            significance = 'Not Significant'\n",
        "            features_to_drop.append(feature_column)\n",
        "        else:\n",
        "            significance = 'Significant'\n",
        "\n",
        "        # Store the results\n",
        "        anova_results.append([feature_column, f_statistic, p_value, significance])\n",
        "\n",
        "# Drop non-significant features\n",
        "df_pre.drop(columns=features_to_drop, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPdluWD1ao-O"
      },
      "outputs": [],
      "source": [
        "# Additional feature reduction with Recursive Feature Elimination (RFE)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Extract the target variable and create X and y\n",
        "y = df_pre['target']\n",
        "X = df_pre.drop(columns=['target'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Specify the number of features to select (you can adjust this)\n",
        "num_features_to_select = 12\n",
        "\n",
        "# Create the RFE model\n",
        "rfe = RFE(estimator=clf, n_features_to_select=num_features_to_select)\n",
        "\n",
        "# Fit the RFE model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_feature_names = X.columns[rfe.support_]\n",
        "\n",
        "# Print the names of the selected features\n",
        "print(\"Selected feature names:\", selected_feature_names.tolist())\n",
        "\n",
        "# Filter both the training and test sets using the selected features\n",
        "X_train_selected = X_train[selected_feature_names]\n",
        "X_test_selected = X_test[selected_feature_names]\n",
        "\n",
        "# Train and evaluate your model using the selected features and the test data\n",
        "clf.fit(X_train_selected, y_train)\n",
        "accuracy = clf.score(X_test_selected, y_test)\n",
        "print(f\"Accuracy on test data with selected features: {accuracy:.2f}\")\n",
        "\n",
        "# Create a final DataFrame including the selected features and the target variable\n",
        "df_final = df_pre[selected_feature_names].copy()\n",
        "df_final['target'] = y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li0WJjebGgnP"
      },
      "outputs": [],
      "source": [
        "# Feature count after reduction\n",
        "\n",
        "unique_column_names = set(df_final.columns)\n",
        "column_count = len(unique_column_names)\n",
        "print(f\"Total Features: {column_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ZkrJAfiDPC"
      },
      "source": [
        "###Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48x7Fhe9cQ4T"
      },
      "outputs": [],
      "source": [
        "# Address class imbalance of the target\n",
        "\n",
        "df_final.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "579_e1t-cQ6z"
      },
      "outputs": [],
      "source": [
        "# Oversample the minority class using \"random over sampling\"\n",
        "# This linear approach supports cleaner interpetation of\n",
        "# feature importance.\n",
        "\n",
        "count_class_0, count_class_1 = df_final.target.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_class_0 = df_final[df_final['target'] == 0] #majority class\n",
        "df_class_1 = df_final[df_final['target'] == 1] #minority class\n",
        "\n",
        "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_over.target.value_counts())\n",
        "\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n69jQigjcQ8-"
      },
      "outputs": [],
      "source": [
        "# Shuffle the array before train/test split\n",
        "\n",
        "df_over = shuffle(df_over,random_state=0)\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsuf0hCDcQ_e"
      },
      "outputs": [],
      "source": [
        "# Train/test splits\n",
        "\n",
        "X, y = df_over, df_over.pop(\"target\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=888)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74QuCnmri3x0"
      },
      "outputs": [],
      "source": [
        "X_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Paoh4TsX2y"
      },
      "source": [
        "##Model Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs6a52Ekb6CZ"
      },
      "source": [
        "###Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ5mHxOYcQ1w"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lr = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLFLxhfx1YvY"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_lr.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OByJr5001Y07"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting\n",
        "\n",
        "# Define the regularization strengths to evaluate\n",
        "C_values = np.logspace(-4, 4, 20)\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each regularization strength\n",
        "for C in C_values:\n",
        "\n",
        "    # Configure the model\n",
        "    model_lr = LogisticRegression(C=C, max_iter=1000)\n",
        "\n",
        "    # Fit model on the training dataset\n",
        "    model_lr.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_lr.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_lr.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('C=%.4f, train: %.3f, test: %.3f' % (C, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs regularization strength\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(C_values, train_scores, label='Train')\n",
        "plt.semilogx(C_values, test_scores, label='Test')\n",
        "plt.title('Model Accuracy vs Regularization Strength')\n",
        "plt.xlabel('Regularization Strength (C)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.3f'))  # Set y-axis formatter\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_N2BIpRgPM8"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "for parameter in model_lr.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUnGYIVhi0WO"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameter dictionary for Logistic Regression\n",
        "\n",
        "param_dict = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid = GridSearchCV(estimator=log_reg,\n",
        "                    param_grid=param_dict,\n",
        "                    cv=5,\n",
        "                    verbose=1,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGiz8Eoei0Y9"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming hyper-parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2mruRKDi0bs"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_lr_u=LogisticRegression(C = 0.0018,\n",
        "                                   penalty ='l1',\n",
        "                                   solver = 'saga')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTetrN6ol96v"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_lr_u.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model_lr_u.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jy5HeLK1cSl"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_lr_u.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnV0Pd-kd5Af"
      },
      "outputs": [],
      "source": [
        "# Get the feature weights (coefficients)\n",
        "weights_lr = model_lr.coef_[0]\n",
        "\n",
        "# Create a DataFrame for easy viewing\n",
        "importance_lr = pd.DataFrame({'Feature': X_train.columns, 'Weight': weights_lr})\n",
        "print(importance_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WarsHx0x1yvv"
      },
      "source": [
        "###Decision Tree Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAmA2I31Wur_"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dt = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zuFXJK3pM6u"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_dt.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoMHDJHHpM9I"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 11)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "\n",
        "  # configure the model\n",
        "  model_dt = DecisionTreeClassifier(max_depth=i, random_state=42)\n",
        "\n",
        "    # fit model on the training dataset\n",
        "  model_dt.fit(X_train, y_train)\n",
        "\n",
        "  # evaluate on the train dataset\n",
        "  train_yhat = model_dt.predict(X_train)\n",
        "  train_acc = accuracy_score(y_train, train_yhat)\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  test_yhat = model_dt.predict(X_test)\n",
        "  test_acc = accuracy_score(y_test, test_yhat)\n",
        "  test_scores.append(test_acc)\n",
        "\n",
        "# summarize progress\n",
        "  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTGReefuou-4"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "model_dt = DecisionTreeClassifier()\n",
        "for parameter in model_dt.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCeSue33ovBJ"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "\n",
        "param_dict = {'criterion' :['gini', 'entropy'],\n",
        "            'max_depth' :range(1,11),\n",
        "            'min_samples_split': [20, 50],\n",
        "            'min_samples_leaf': [20, 50],\n",
        "            'ccp_alpha': [0.0001, 0.001]\n",
        "              }\n",
        "\n",
        "tree_class = DecisionTreeClassifier()\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 5,\n",
        "        verbose= 1,\n",
        "        n_jobs = -1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3aAZ1g5ovET"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming hyper-parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKNc5NNCnsuf"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_dt_u=DecisionTreeClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='gini',\n",
        "                                   max_depth = 8,\n",
        "                                   min_samples_leaf = 20,\n",
        "                                   min_samples_split = 20\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THUeLYtSPHKx"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_dt_u.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model_dt_u.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7vq_gqFN5Gx"
      },
      "outputs": [],
      "source": [
        "# Look again for overfitting via Accuracy\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 9)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "\n",
        "  # configure the model\n",
        "  model_dt_u = DecisionTreeClassifier(max_depth=i, random_state=888)\n",
        "\n",
        "    # fit model on the training dataset\n",
        "  model_dt_u.fit(X_train, y_train)\n",
        "\n",
        "  # evaluate on the train dataset\n",
        "  train_yhat = model_dt_u.predict(X_train)\n",
        "  train_acc = accuracy_score(y_train, train_yhat)\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  test_yhat = model_dt_u.predict(X_test)\n",
        "  test_acc = accuracy_score(y_test, test_yhat)\n",
        "  test_scores.append(test_acc)\n",
        "\n",
        "# summarize progress\n",
        "  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9oWbgjpvrM6"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_dt_u.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed5fWAF0c8Mc"
      },
      "outputs": [],
      "source": [
        "# Get the feature importances\n",
        "importance_dt = model_dt_u.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy viewing\n",
        "importance_dt = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance_dt})\n",
        "print(importance_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeK_MFmyVsRp"
      },
      "source": [
        "### Random Forrest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oilo2OhVrpl"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rf = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzfsCHsld0Xz"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_rf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa86R1rGd0aS"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = range(1, 11)\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Configure the model with the current max_depth\n",
        "    model_rf = RandomForestClassifier(max_depth=i, random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_rf.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_rf.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('Max depth: %d, Train Accuracy: %.3f, Test Accuracy: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.xlabel('Max Depth of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Max Depth for RandomForest')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYaYbh5Ad0cj"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "for parameter in model_rf.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910poJNGv239"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "\n",
        "param_dict = {\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"max_depth\" :range(1,11),\n",
        "    'min_samples_split': [20, 50],\n",
        "    'min_samples_leaf': [20, 50],\n",
        "    'max_features': ['auto'],\n",
        "    'ccp_alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "tree_class = RandomForestClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 5,\n",
        "        verbose= 1,\n",
        "        n_jobs = -1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZGQ5tw-wQwr"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming hyper-parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82e-T5SSwQzh"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_rf_u = RandomForestClassifier(\n",
        "                                  ccp_alpha = 0.001,\n",
        "                                   criterion ='gini',\n",
        "                                   max_depth = 4,\n",
        "                                   max_features = 'auto',\n",
        "                                   min_samples_leaf = 50,\n",
        "                                   min_samples_split = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK6-Wp1oWG7_"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_rf_u.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_rf_u.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUjhCQj8WG-s"
      },
      "outputs": [],
      "source": [
        "# Look again for overfitting via Accuracy\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = range(1, 11)\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Configure the model with the current max_depth\n",
        "    model_rf_u = RandomForestClassifier(max_depth=i, random_state=888)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model_rf_u.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_rf_u.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_rf_u.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('Max depth: %d, Train Accuracy: %.3f, Test Accuracy: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.xlabel('Max Depth of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Max Depth for RandomForest')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTQzkmK0WHCO"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_rf_u.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vicO78UkfVQw"
      },
      "outputs": [],
      "source": [
        "# Get the feature importances\n",
        "importance_rf = model_rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy viewing\n",
        "importance_rf = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance_rf})\n",
        "print(importance_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEmHEooTsei6"
      },
      "source": [
        "##Gradient Boost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuWMkNz1c01h"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "model_gb = GradientBoostingClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEcpXoR4ulte"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "model_gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_gb.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGxo-I-Zultf"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = range(1, 11)\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Initialize the Gradient Boosting Classifier with varying max_depth\n",
        "    model_gb = GradientBoostingClassifier(max_depth=i, random_state=42)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model_gb.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_gb.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_gb.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('Max depth: %d, Train Accuracy: %.3f, Test Accuracy: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.xlabel('Max Depth of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Gradient Boosting Model Accuracy vs Tree Depth')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1mOsNLZultf"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "for parameter in model_gb.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIgR3QjsucI3"
      },
      "outputs": [],
      "source": [
        "# Potentially faster Grid Search for GB model\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "param_dict = {\n",
        "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
        "    \"max_depth\": range(1, 11),\n",
        "    #'min_samples_split': [20, 50],\n",
        "    #'min_samples_leaf': [20, 50],\n",
        "    'ccp_alpha': [0.0001, 0.001],\n",
        "    'n_estimators': [100, 200, 300],  # Example values\n",
        "    'learning_rate': [0.01, 0.1]  # Example values\n",
        "}\n",
        "\n",
        "tree_class = GradientBoostingClassifier()\n",
        "\n",
        "# Use RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=tree_class,\n",
        "                                   param_distributions=param_dict,\n",
        "                                   n_iter=100,\n",
        "                                   cv=5,\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1,\n",
        "                                   random_state=42)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILTN-jf9ultf"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "\n",
        "param_dict = {\n",
        "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
        "    \"max_depth\": range(1, 11),\n",
        "    #'min_samples_split': [20, 50], # reducing compute time\n",
        "    #'min_samples_leaf': [20, 50], # reducing compute time\n",
        "    'ccp_alpha': [0.0001, 0.001],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "tree_class = GradientBoostingClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 5,\n",
        "        verbose= 1,\n",
        "        n_jobs = -1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRGXPAaEg9o5"
      },
      "outputs": [],
      "source": [
        "# Extract the best preforming tested parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiPpBUYDU8vH"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_gb_u = GradientBoostingClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='friedman_mse',\n",
        "                                   max_depth = 4,\n",
        "                                   min_samples_split = 20,\n",
        "                                   min_samples_leaf = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQYdYbIAultg"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_gb_u.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model_gb_u.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btEQ5RpfYcr5"
      },
      "outputs": [],
      "source": [
        "# Look again for overfitting via Accuracy\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = range(1, 11)\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Configure the model with the current max_depth\n",
        "    model_gb_u = RandomForestClassifier(max_depth=i, random_state=888)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    model_gb_u.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_gb_u.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_gb_u.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('Max depth: %d, Train Accuracy: %.3f, Test Accuracy: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.xlabel('Max Depth of Trees')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Max Depth for RandomForest')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItYlmIU-ultg"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# fit a model\n",
        "model_gb_u.fit(X_train, y_train.values)\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_gb_u.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOBHo0Ilf_aK"
      },
      "outputs": [],
      "source": [
        "# Get the feature importances\n",
        "importance_gb = model_gb.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy viewing\n",
        "importance_gb = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance_gb})\n",
        "print(importance_gb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586FyZWtBB0W"
      },
      "source": [
        "##Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1PunCcpUiE"
      },
      "source": [
        "###LIME (Local Interpretable Model-agnostic Explanations)\n",
        "\n",
        "LIME Pros:  Model Agnostic: Works with any machine learning model.\n",
        "Local Interpretation: Provides explanations for individual predictions, offering insight into how the model behaves for single instances.\n",
        "Simplicity: Generates simple, interpretable models (like linear models) that approximate the predictions locally.\n",
        "Speed: Can be faster than SHAP for individual explanations because it does not require a complete background dataset to make comparisons.  \n",
        "\n",
        "LIME Cons:\n",
        "Local Scope: Explanations are local and might not reflect the model's behavior in other areas of the feature space.\n",
        "Stability: Explanations can sometimes be unstable, meaning similar instances can have significantly different explanations.\n",
        "Representativity: The perturbation process may generate new synthetic samples that are not representative of the actual data distribution.\n",
        "Complexity: Understanding and selecting the appropriate configuration can be complex and requires a good understanding of the model and the data.  \n",
        "\n",
        "###SHAP (SHapley Additive exPlanations)\n",
        "SHAP Pros:  Solid Theoretical Foundation: Based on Shapley values from cooperative game theory, providing a fair distribution of \"payout\" among features.\n",
        "Consistency: Guarantees that feature contributions are consistent with changes in the model output.\n",
        "Global Interpretation: In addition to local explanations, SHAP values can be aggregated to provide a global view of feature importance.\n",
        "Feature Interaction: SHAP can capture and show feature interactions.  \n",
        "\n",
        "SHAP Cons:\n",
        "Computationally Intensive: Can be slower than LIME, especially for models with many features or complex models like ensemble methods or deep learning.\n",
        "Complex Outputs: The interpretations can sometimes be more difficult to understand due to complex interactions and many contributing factors.\n",
        "High Dimensionality: SHAP's computational complexity can become problematic with high-dimensional datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihM8GljGUw-R"
      },
      "outputs": [],
      "source": [
        "# Print Global SHAP values\n",
        "\n",
        "import shap as shap\n",
        "from shap import TreeExplainer, summary_plot\n",
        "\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "exp = TreeExplainer(model_dt)\n",
        "sv = exp.shap_values(X_test)\n",
        "\n",
        "# Initialize an empty list to store feature statistics\n",
        "feature_stats = []\n",
        "feature_to_drop = []\n",
        "\n",
        "# Loop through each feature and calculate min, mean, and max SHAP values\n",
        "for i in range(len(X_test.columns)):\n",
        "    feature_name = X_test.columns[i]\n",
        "    shap_values_feature = sv[1][:, i]\n",
        "    min_val = np.min(shap_values_feature)\n",
        "    mean_val = np.mean(shap_values_feature)\n",
        "    max_val = np.max(shap_values_feature)\n",
        "    feature_stats.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "\n",
        "    # Append the statistics to the list of features to drop\n",
        "    # if 1==1:#not (min_val == mean_val == max_val == 0):\n",
        "    if (min_val == mean_val == max_val == 0):\n",
        "        feature_to_drop.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "# Create a DataFrames of all feature with their min, mean, max values\n",
        "feature_stats_df = pd.DataFrame(feature_stats, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_stats.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_stats_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Create a DataFrames of dropped features\n",
        "feature_to_drop_df = pd.DataFrame(feature_to_drop, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_to_drop_df.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_to_drop_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Plots\n",
        "# Display the SHAP summary plot for all features\n",
        "shap.summary_plot(sv[1], X_test, max_display=20)\n",
        "\n",
        "# Plotting the mean SHAP values for each feature\n",
        "plt.figure(figsize=(10, len(feature_stats_df) / 2))  # Adjust the figure size as needed\n",
        "feature_stats_df.sort_values(by='Mean SHAP Value', ascending=True, inplace=True)\n",
        "plt.barh(feature_stats_df['Feature'], feature_stats_df['Mean SHAP Value'], color='skyblue')\n",
        "plt.xlabel('Mean SHAP Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Mean SHAP Values for Each Feature')\n",
        "plt.show()\n",
        "\n",
        "print(feature_stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_561r2wDRb3"
      },
      "outputs": [],
      "source": [
        "# Force Plot visualization: single customer (Local SHAP values)\n",
        "\n",
        "# Calculate SHAP values for the first row of the X_test dataset\n",
        "shap_values_single = exp.shap_values(X_test.iloc[120]) # Good Exxample of really bad churn\n",
        "\n",
        "# Initialize JavaScript visualization in Google Colab (even if it might not display)\n",
        "shap.initjs()\n",
        "\n",
        "# Generate a force plot using matplotlib backend\n",
        "plt.figure(figsize=(40, 10))  # You can adjust the size as needed\n",
        "shap.force_plot(exp.expected_value[1], shap_values_single[1], X_test.iloc[0], feature_names=X_test.columns, matplotlib=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4eHaz2XkhfQ"
      },
      "outputs": [],
      "source": [
        "# Filter customers based on all specified features and values simultaneously\n",
        "\n",
        "\n",
        "high_value_features = {\n",
        "    \"product_cpe_6\": 1,\n",
        "    \"product_sku_2\": 1\n",
        "    #\"sales_team_3\": 1\n",
        "    #\"customer_demo_age_gen_boomers\" : 1\n",
        "    #\"sales_program_18\" : 1\n",
        "                       }\n",
        "\n",
        "filtered_customers = X_test.copy()\n",
        "\n",
        "for feature, value in high_value_features.items():\n",
        "    filtered_customers = filtered_customers[filtered_customers[feature] == value]\n",
        "\n",
        "# DataFrame to store SHAP values\n",
        "shap_values_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over filtered customers to calculate SHAP values and generate force plots\n",
        "for index, row in filtered_customers.iterrows():\n",
        "    # Calculate SHAP values for the current example\n",
        "    shap_values_example = exp.shap_values(row)\n",
        "\n",
        "    # Append the SHAP values to the DataFrame\n",
        "    shap_row_df = pd.DataFrame([shap_values_example[1]], columns=[f'shap_{col}' for col in X_test.columns])\n",
        "    shap_values_df = pd.concat([shap_values_df, shap_row_df], ignore_index=True)\n",
        "\n",
        "    # Generate a force plot for the example\n",
        "    shap.initjs()\n",
        "    shap.force_plot(exp.expected_value[1], shap_values_example[1], row, matplotlib=True)\n",
        "    plt.show()\n",
        "\n",
        "# Concatenate the SHAP values DataFrame with the filtered customers DataFrame\n",
        "final_df = pd.concat([filtered_customers.reset_index(drop=True), shap_values_df], axis=1)\n",
        "\n",
        "# Export to CSV\n",
        "final_df.to_csv('shap_values.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5GvheCOs192"
      },
      "outputs": [],
      "source": [
        "# Feature Importance from the Gradient model\n",
        "\n",
        "feature_importances = model_gb.feature_importances_\n",
        "\n",
        "# To display feature importance\n",
        "for i, feature in enumerate(X_train.columns):\n",
        "    print(f'Feature: {feature}, Importance: {feature_importances[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHehZeblBIeV"
      },
      "source": [
        "##Model Interpretation and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All models resulted in similar findings. However, Random Forrest and Gradient Boosted Classification models had compute needs that made them problematic. When a Decision Tree Classifier is property tuned, its' tendency to overfit is controlled. As it is fast and easy to extract feature importance, it is my model of choice for this exercise.\n",
        "\n",
        "Product_sku_2 consistently showed itself to be the most powerful indicator of churn, trumping all other features. The conclusion is that this feature is the \"tide that raises all ships\" and one that can be implemented broadly. In my commercial implementation of these findings, it was the primary focus for mitigating churn and resulted in a > 15% reduction in measured monthly churn.\n",
        "\n",
        "Having said that, a closer examination of the interaction between the complete set of features for a particular customer, including demographic, sales channel, customer premise equipment and kpis shows that product_sku_2 is only a portion of the story. It should not be conclude that product_sku_2 will nullify any/all other factors for a particular customer."
      ],
      "metadata": {
        "id": "8QCFesiUgqik"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-jzjP7Im4rn"
      },
      "source": [
        "##Appendix and Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7y0u4XpJs-4"
      },
      "source": [
        "Selecting the appropriate value to plot train vs test to indenitfy potential ofverfitting.\n",
        "\n",
        "**Accuracy**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Simplicity: Accuracy is straightforward to understand and interpret. It's the percentage of correctly classified instances out of all instances.\n",
        "Usefulness in Balanced Datasets: In cases where classes are balanced, accuracy can be a reliable measure of model performance.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Misleading in Imbalanced Datasets: In situations where there is a significant class imbalance, accuracy can be misleading. A model could predict the majority class for all instances and still achieve high accuracy.\n",
        "No Insight into Type I/II Errors: Accuracy doesn't distinguish between the types of errors (false positives and false negatives).\n",
        "<br>\n",
        "<br>\n",
        "**F1-Score**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Balance Between Precision and Recall: F1-score provides a balance between precision and recall. It is especially useful in cases where we need to balance false positives and false negatives.\n",
        "Better for Imbalanced Datasets: It is more informative than accuracy in case of an imbalanced dataset.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "More Complex to Understand: F1-score is not as intuitive as accuracy, especially for non-technical stakeholders.\n",
        "Not a Single Error Type Focus: If your specific problem requires optimizing specifically for either precision or recall (but not both), F1-score might not be the best metric.\n",
        "<br>\n",
        "<br>\n",
        "**ROC-AUC**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Performance Across Thresholds: AUC-ROC measures the model's performance across all classification thresholds, providing a comprehensive view of its effectiveness.\n",
        "Useful for Imbalanced Datasets: Like F1-score, it is more informative than accuracy for imbalanced classes.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Can Be Overly Optimistic: In highly imbalanced datasets, ROC-AUC might present an overly optimistic view of the model’s performance.\n",
        "Complexity in Interpretation: Understanding and explaining ROC curves and AUC can be more complex compared to straightforward metrics like accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFzSVLOzO29q"
      },
      "source": [
        "Gini Impurity\n",
        "Pros:\n",
        "\n",
        "Faster Computation: Gini impurity is computationally less intensive as it doesn't involve logarithmic calculations, which can be advantageous for large datasets.\n",
        "Performance: It tends to work well in practice and is the default in many decision tree algorithms, like in scikit-learn's DecisionTreeClassifier.\n",
        "Cons:\n",
        "\n",
        "Less Sensitive to Changes in Class Probabilities: Gini impurity might be less sensitive to probability changes of the minority class, as it squares the probability terms.\n",
        "Entropy (Information Gain)\n",
        "Pros:\n",
        "\n",
        "Sensitivity to Class Probability Changes: Entropy is more sensitive to changes in the class probabilities of the nodes, potentially leading to more balanced trees.\n",
        "Information Theoretic Model: It has a basis in information theory, providing a clear interpretation in terms of information content and uncertainty.\n",
        "Cons:\n",
        "\n",
        "Computational Intensity: Calculating entropy involves logarithmic computations, which can be more computationally intensive than Gini impurity, especially for very large datasets.\n",
        "Can Lead to Overfitting: In some cases, because entropy is more sensitive to class probabilities, it can lead to models that are more complex and potentially overfitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxRxwA3ztaT2"
      },
      "source": [
        "Bayesian-Optimization Library:\n",
        "\n",
        "Pro: Offers a probabilistic model that can efficiently find the optimal parameters, especially useful when the number of experiments is limited.\n",
        "Con: Might be less efficient in high-dimensional space and requires careful choice of the prior.\n",
        "Scikit-Optimize:\n",
        "\n",
        "Pro: Integrates seamlessly with scikit-learn and offers several methods including Bayesian optimization, which is useful for optimizing expensive-to-evaluate functions.\n",
        "Con: Limited to optimization tasks and might not be as scalable as some other libraries.\n",
        "GPyOpt:\n",
        "\n",
        "Pro: Built on Gaussian Process models, it is excellent for fine-tuning where evaluations of the function are expensive.\n",
        "Con: Can be slower and less practical for large-scale hyperparameter optimization due to the computational cost of Gaussian Processes.\n",
        "Hyperopt:\n",
        "\n",
        "Pro: Uses Bayesian optimization and supports parallelization, making it efficient for large searches.\n",
        "Con: Can be complex to configure and understand, especially for beginners.\n",
        "SHERPA:\n",
        "\n",
        "Pro: Designed for hyperparameter tuning of machine learning models, supports a variety of algorithms, and is easy to use.\n",
        "Con: Might not offer as wide a range of optimization algorithms compared to some other tools.\n",
        "Optuna:\n",
        "\n",
        "Pro: A modern library with an easy-to-use interface, it offers efficient and flexible optimization with visualization features.\n",
        "Con: Its flexibility might come at the cost of a steeper learning curve for advanced features.\n",
        "Ray Tune:\n",
        "\n",
        "Pro: Highly scalable, supports a wide range of optimization algorithms, and integrates well with deep learning frameworks.\n",
        "Con: Its broad functionality can make it more complex to set up and use.\n",
        "Neural Network Intelligence (NNI):\n",
        "\n",
        "Pro: Designed for neural networks, it offers a rich set of tuning strategies and easy integration with popular deep learning frameworks.\n",
        "Con: More focused on neural networks, might be less applicable for other types of models.\n",
        "MLMachine:\n",
        "\n",
        "Pro: Provides an easy-to-use framework for machine learning workflows, including hyperparameter tuning.\n",
        "Con: Less known and might lack some advanced features or optimizations found in more established libraries.\n",
        "Talos:\n",
        "\n",
        "Pro: Specifically designed for Keras models, making it very convenient for users of this framework.\n",
        "Con: Limited to Keras, which might not be suitable if you're using different machine learning frameworks.\n",
        "GridSearchCV:\n",
        "\n",
        "Pro: Part of scikit-learn, very straightforward and easy to use for exhaustive search over specified parameter values.\n",
        "Con: Computationally expensive as it evaluates all possible combinations and not efficient for large hyperparameter spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmJDF_uI1Rkh"
      },
      "source": [
        "## Parking lot of code to revisit time allowing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo_032UpQhT2"
      },
      "outputs": [],
      "source": [
        "# Graphical analysis of trees\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "# Train a decision tree model\n",
        "model = DecisionTreeClassifier(max_depth=3)  # Limit depth for visualization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Export as dot file\n",
        "dot_data = export_graphviz(model, out_file=None,\n",
        "                           feature_names=X_train.columns,\n",
        "                           class_names=['0', '1'],  # Assuming binary classification with classes '0' and '1'\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                           proportion=False,  # Set to 'True' to show percentages instead of sample counts\n",
        "                           precision=2,  # Set precision for floating point numbers\n",
        "                           label='all',  # Use 'root' to show labels at the root node or 'all' to show at all nodes\n",
        "                           leaves_parallel=False)  # Set to 'True' to align leaf nodes horizontally\n",
        "\n",
        "# Create graph from dot data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Render and show the graph\n",
        "Image(graph.pipe(format='png'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXY240ONLPw6"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting via F1-score\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 15)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Configure the model\n",
        "    model_dtc = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # Fit model on the training dataset\n",
        "    model_dtc.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_dtc.predict(X_train)\n",
        "    train_f1 = f1_score(y_train, train_yhat, average='binary')  # Modify for binary/multiclass\n",
        "    train_scores.append(train_f1)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_dtc.predict(X_test)\n",
        "    test_f1 = f1_score(y_test, test_yhat, average='binary')  # Modify for binary/multiclass\n",
        "    test_scores.append(test_f1)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('>%d, train: %.3f, test: %.3f' % (i, train_f1, test_f1))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14eeOjLjhLmVNTrPOGXUvFPbjkXKAlfJe",
      "authorship_tag": "ABX9TyMOKSqJ0fs8ACNqTtdPfhGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}