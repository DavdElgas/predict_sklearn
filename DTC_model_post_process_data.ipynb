{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david91107/predict_sklearn/blob/main/DTC_model_post_process_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Jupyter notebook, I present a sophisticated AI model designed to identify and quantify the key factors contributing to customer churn in subscription-based services. This model epitomizes my adeptness in leveraging advanced machine learning techniques and data analytics to solve real-world business problems. By meticulously preprocessing the dataset, employing feature engineering, and experimenting with various machine learning algorithms, I have developed a model that not only predicts churn with high accuracy but also provides insightful interpretations of the underlying causes. This project demonstrates my proficiency in Python programming, deep understanding of predictive modeling, and ability to translate complex data insights into actionable business strategies. It stands as a testament to my capacity to add value in a Data Science role, where analytical rigor and practical problem-solving skills are paramount."
      ],
      "metadata": {
        "id": "bmPOAm2W_I9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive churn model for a subscription service."
      ],
      "metadata": {
        "id": "D3bBVbNF7M3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Project Introduction\n",
        "*   Overview of the project goal\n",
        "*   Importance of churn prediction in subscription-based service\n",
        "<br>\n",
        "<br>\n",
        "2.   Data Ingestion\n",
        "\n",
        "*   Sourcing the data (APIs, databases, CSV files, etc.)\n",
        "*   Loading data into the environment\n",
        "*   Data Cleaning and Preprocessing\n",
        "<br>\n",
        "<br>\n",
        "3.   Handling missing values\n",
        "*   Data type conversion (if necessary)\n",
        "*   Outlier detection and treatment\n",
        "*   Exploratory Data Analysis (EDA)\n",
        "<br>\n",
        "<br>\n",
        "4.   Statistical summaries of the dataset\n",
        "*   Visualization of key metrics and relationships\n",
        "*   Initial insights and identification of potential predictors of churn\n",
        "*   Feature Engineering\n",
        "<br>\n",
        "<br>\n",
        "5.   Creation of new features based on existing data\n",
        "*   Feature transformation (scaling, encoding categorical variables, etc.)\n",
        "*   Feature selection for the model\n",
        "<br>\n",
        "<br>\n",
        "6.   Model Selection\n",
        "*   Overview of candidate machine learning models\n",
        "*   Criteria for model selection (accuracy, interpretability, etc.)\n",
        "<br>\n",
        "<br>\n",
        "7.   Model Training and Validation\n",
        "*   Splitting data into training and testing sets\n",
        "*   Model training and hyperparameter tuning\n",
        "*   Model validation and performance assessment (using metrics like accuracy, precision, recall, F1-score, ROC-AUC, etc.)\n",
        "<br>\n",
        "<br>\n",
        "8.   Model Interpretation and Insights\n",
        "*   Importance of features in the model\n",
        "*   Interpretation of the model’s predictions\n",
        "*   Business insights drawn from the model’s results\n",
        "<br>\n",
        "<br>\n",
        "9.   Conclusion and Recommendations\n",
        "*   Summary of findings\n",
        "*   Potential strategies to reduce customer churn based on insights\n",
        "*   Future work and improvements for the model\n",
        "<br>\n",
        "<br>\n",
        "10.   Appendices and References\n",
        "*   Code documentation\n",
        "*   References to libraries and frameworks used\n",
        "*   Additional resources"
      ],
      "metadata": {
        "id": "DyCwjxuU9g6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Introduction"
      ],
      "metadata": {
        "id": "gzPjStWb_OiW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYnXMx8khjtI"
      },
      "source": [
        "##Data Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling missing values"
      ],
      "metadata": {
        "id": "tK0g5Ao0ApJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Statistical summaries of the dataset"
      ],
      "metadata": {
        "id": "586FyZWtBB0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creation of new features based on existing data"
      ],
      "metadata": {
        "id": "UR9iWVfbBGb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Selection"
      ],
      "metadata": {
        "id": "vvk4En_jBIbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Interpretation and Insights"
      ],
      "metadata": {
        "id": "CHehZeblBIeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion and Recommendations"
      ],
      "metadata": {
        "id": "E1COY1W0BgqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendices and References"
      ],
      "metadata": {
        "id": "s3DWz-c8Bj3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Libraries"
      ],
      "metadata": {
        "id": "2xxrkQj4X2BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install pandas==1.5.3 # to be compatible with google-colab 1.0.0\n",
        "!pip install matplotlib\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "R-rqyBKdYMol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Methods"
      ],
      "metadata": {
        "id": "ozpJD7MXYTcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3xDY_eywU3s"
      },
      "outputs": [],
      "source": [
        "# Import necessary methods\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning) # Ignore FutureWarning\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score,f1_score\n",
        "\n",
        "import shap as shap\n",
        "from shap import TreeExplainer, summary_plot\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingest Data"
      ],
      "metadata": {
        "id": "n0KLNmEAYlT-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arUD5_dlBU8f"
      },
      "outputs": [],
      "source": [
        "# Load CSV data file into a dataframe\n",
        "\n",
        "df_pre = pd.read_csv(r'/content/drive/MyDrive/Data_sets/telco_subscription_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfwlLg5F0AwD"
      },
      "outputs": [],
      "source": [
        "# Check data load\n",
        "\n",
        "df_pre.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleansing"
      ],
      "metadata": {
        "id": "Oo6xva2IYvuw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXrgggFBVD7"
      },
      "outputs": [],
      "source": [
        "# Look for Null values\n",
        "\n",
        "print('Null count')\n",
        "print(df_pre.isnull().sum())\n",
        "\n",
        "print()\n",
        "\n",
        "# Look for NaN values\n",
        "print('NaN count')\n",
        "print(df_pre.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXE2rTFcE1kd"
      },
      "outputs": [],
      "source": [
        "# Drop Nulls\n",
        "\n",
        "df_pre.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any duplication\n",
        "\n",
        "print(\"Number of duplicate rows:\", df_pre.duplicated().sum())"
      ],
      "metadata": {
        "id": "0w255ARjj5Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop random_key\n",
        "\n",
        "df_pre.drop(columns='random_key', inplace=True)"
      ],
      "metadata": {
        "id": "pVfjjnzFMgUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the int values are actually string values\n",
        "# that will need to be flattened\n",
        "\n",
        "columns_to_cast = ['sales_team', 'sales_channel','sales_program','product_sku','product_cpe','customer_demo_geography_1','customer_demo_geography_2']\n",
        "\n",
        "df_pre[columns_to_cast] = df_pre[columns_to_cast].astype(object)"
      ],
      "metadata": {
        "id": "ykpXjPXngPpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some of the float64 values are actually int values\n",
        "# that will need to be flattened\n",
        "\n",
        "# Define a list of column names to cast\n",
        "columns_to_cast = ['product_operations_kpi1']\n",
        "\n",
        "# Cast the columns\n",
        "df_pre[columns_to_cast] = df_pre[columns_to_cast].astype(int)"
      ],
      "metadata": {
        "id": "iiZjesRThGRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuDaRLaWQsPO"
      },
      "outputs": [],
      "source": [
        "# Count distinct values for categorical features\n",
        "\n",
        "features_categorical = ['product_sku','customer_demo_geography_2','customer_demo_geography_1','sales_channel','product_cpe','sales_program','sales_team']\n",
        "\n",
        "for feature in features_categorical:\n",
        "  unique_values = df_pre[feature].nunique()\n",
        "  print(feature, unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop categories with a high unique value count until an ANOVA check\n",
        "\n",
        "feature_high_count = ['customer_demo_geography_2']\n",
        "df_pre.drop(columns=feature_high_count, inplace=True)"
      ],
      "metadata": {
        "id": "wxsGF0lP7wQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explority Data Analysis"
      ],
      "metadata": {
        "id": "B1ht5aDeZaZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfmNal4c_-Y"
      },
      "outputs": [],
      "source": [
        "# Exploratory plots\n",
        "\n",
        "df_pre.hist(bins=50, figsize=(10, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7H-bFoXBVHv"
      },
      "outputs": [],
      "source": [
        "# Collinearity: Pearson\n",
        "\n",
        "df_num = df_pre.select_dtypes(include = ['float64', 'int64'])\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "corrMatrix = df_num.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collinearity: VIF\n",
        "\n",
        "df_num = df_pre.select_dtypes(include = ['float64', 'int64'])\n",
        "\n",
        "# Calculate VIF for each numeric feature\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = df_num.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(df_num.values, i) for i in range(df_num.shape[1])]\n",
        "\n",
        "# Print the VIF values to identify multicollinearity among numeric features\n",
        "print(\"VIF Values for Numeric Features:\")\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "q-EK8EYlb6Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "Sx0z-_k-1b1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEJ8EwXodALz"
      },
      "outputs": [],
      "source": [
        "# Normalize continuous features\n",
        "\n",
        "columns_to_norm = ['product_usage_kpi1','product_usage_kpi2', 'product_usage_kpi3','product_operations_kpi1','product_operations_kpi2']\n",
        "\n",
        "for column in columns_to_norm:\n",
        "  df_pre[column] = df_pre[column] / df_pre[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT2fzXK-dABP"
      },
      "outputs": [],
      "source": [
        "# Bin age into generations\n",
        "# Assumption is that trends fall along generations more than specific age\n",
        "# https://www.beresfordresearch.com/age-range-by-generation/\n",
        "\n",
        "df_pre['customer_demo_age_gen'] = df_pre['customer_demo_age'].apply(lambda age:\n",
        "    'gen_z' if age >= 12 and age <= 27 else\n",
        "    'millennial' if age >= 28 and age <= 43 else\n",
        "    'gen_x' if age >= 44 and age <= 59 else\n",
        "    'boomers' if age >= 60 and age <= 78 else\n",
        "    'post_war' if age >= 79 and age <= 96 else\n",
        "    'invalid' if age < 18 and age > 99 else\n",
        "    'other'\n",
        ")\n",
        "\n",
        "# Drop age\n",
        "df_pre.drop('customer_demo_age', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pre.info()"
      ],
      "metadata": {
        "id": "Er5A2jrr6L87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lRQpGA-dAOY"
      },
      "outputs": [],
      "source": [
        "# Flatten categorical features\n",
        "\n",
        "categorical_features = ['product_sku','sales_channel','sales_team','product_cpe','sales_program','customer_demo_age_gen','customer_demo_geography_1']\n",
        "\n",
        "# Perform one-hot encoding (flattening) of the specified columns\n",
        "df_pre = pd.get_dummies(df_pre, columns=categorical_features)\n",
        "\n",
        "# Convert boolean values to integers (1 and 0)\n",
        "df_pre = df_pre.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Reduction"
      ],
      "metadata": {
        "id": "SI_vp3Pc06bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-reduction feature data\n",
        "\n",
        "unique_column_names = set(df_pre.columns)\n",
        "column_count = len(unique_column_names)\n",
        "print(f\"Total Features: {column_count}\")\n",
        "\n",
        "# For a list of features\n",
        "#print()\n",
        "#for column, dtype in df_pre.dtypes.items():\n",
        "#    print(f'Column: {column}, Data Type: {dtype}')"
      ],
      "metadata": {
        "id": "zZdXcM4hzkfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-gRM3M67gR_"
      },
      "outputs": [],
      "source": [
        "# Reduce features to those that are most\n",
        "# statistically significant based on ANOVA\n",
        "\n",
        "# Define the target variable (column 1)\n",
        "target_variable = 'target'\n",
        "\n",
        "# Perform ANOVA tests for each feature\n",
        "alpha = 0.05\n",
        "\n",
        "# Initialize a list to store ANOVA results\n",
        "anova_results = []\n",
        "\n",
        "for feature_column in df_pre.columns[2:]:\n",
        "  f_statistic, p_value = f_oneway(df_pre[feature_column], df_pre[target_variable])\n",
        "  if p_value >= alpha:\n",
        "    significance = 'Not Significant'\n",
        "    df_pre.drop(columns=feature_column, inplace=True)\n",
        "\n",
        "  else:\n",
        "    significance = 'Significant'\n",
        "\n",
        "  anova_results.append([feature_column, f_statistic, p_value, significance])\n",
        "\n",
        "# Create a DataFrame from the ANOVA results\n",
        "df_anova = pd.DataFrame(anova_results, columns=['Feature', 'F-statistic', 'P-value', 'Significance'])\n",
        "df_filtered_not_sig = df_anova[df_anova['Significance'] == 'Not Significant'][['Feature', 'Significance']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature count after reduction\n",
        "\n",
        "unique_column_names = set(df_pre.columns)\n",
        "column_count = len(unique_column_names)\n",
        "print(f\"Total Features: {column_count}\")\n",
        "\n",
        "# For a list of features\n",
        "#print()\n",
        "#for column, dtype in df_pre.dtypes.items():\n",
        "#    print(f'Column: {column}, Data Type: {dtype}')"
      ],
      "metadata": {
        "id": "Li0WJjebGgnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resuling feature set is still too large to be practical."
      ],
      "metadata": {
        "id": "wIRlr3y9ag4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional feature reduction using Recursive Feature Elimination\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset into a DataFrame (replace 'your_dataset.csv' with your actual dataset file)\n",
        "df_rfe = df_pre.copy()\n",
        "\n",
        "# Extract the target variable and create X and y\n",
        "y = df_rfe.pop('target')\n",
        "X = df_rfe\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Specify the number of features to select (you can adjust this)\n",
        "num_features_to_select = 12\n",
        "\n",
        "# Create the RFE model\n",
        "rfe = RFE(estimator=clf, n_features_to_select=num_features_to_select)\n",
        "\n",
        "# Fit the RFE model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected feature indices\n",
        "selected_features = [i for i, selected in enumerate(rfe.support_) if selected]\n",
        "\n",
        "# Print the selected feature indices\n",
        "print(\"Selected feature indices:\", selected_features)\n",
        "\n",
        "# Get the names of all features\n",
        "all_feature_names = X.columns.tolist()\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = [all_feature_names[i] for i in selected_features]\n",
        "\n",
        "# Print the names of the selected features\n",
        "print(\"Selected feature names:\", selected_feature_names)\n",
        "\n",
        "# Now, you can train and evaluate your model using the selected features and the test data\n",
        "clf.fit(X_train.iloc[:, selected_features], y_train)\n",
        "accuracy = clf.score(X_test.iloc[:, selected_features], y_test)\n",
        "print(f\"Accuracy on test data with selected features: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "zPdluWD1ao-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional feature reduction using Recursive Feature Elimination\n",
        "# Plot\n",
        "\n",
        "df_rfe = df_pre.copy()\n",
        "\n",
        "# Extract the target variable and create X and y\n",
        "y = df_rfe.pop('target')\n",
        "X = df_rfe\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Specify the maximum number of features\n",
        "max_features_to_select = min(X_train.shape[1], 30)  # You can adjust the maximum number of features\n",
        "\n",
        "# Initialize lists to store results\n",
        "selected_feature_counts = []\n",
        "accuracies = []\n",
        "\n",
        "# Iterate over different numbers of selected features\n",
        "for num_features_to_select in range(1, max_features_to_select + 1):\n",
        "    # Create the RFE model\n",
        "    rfe = RFE(estimator=clf, n_features_to_select=num_features_to_select)\n",
        "\n",
        "    # Fit the RFE model to the training data\n",
        "    rfe.fit(X_train, y_train)\n",
        "\n",
        "    # Get the selected feature indices\n",
        "    selected_features = [i for i, selected in enumerate(rfe.support_) if selected]\n",
        "\n",
        "    # Now, you can train and evaluate your model using the selected features and the test data\n",
        "    clf.fit(X_train.iloc[:, selected_features], y_train)\n",
        "    accuracy = clf.score(X_test.iloc[:, selected_features], y_test)\n",
        "\n",
        "    # Append results to the lists\n",
        "    selected_feature_counts.append(num_features_to_select)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plot the accuracy vs. the number of selected features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(selected_feature_counts, accuracies, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Selected Features\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs. Number of Selected Features\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3X1_WwphJGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rfe = df_pre[['target','product_sku_1', 'product_sku_2', 'sales_channel_0', 'sales_team_3', 'sales_team_17', 'product_cpe_6', 'product_cpe_7', 'sales_program_18', 'customer_demo_age_gen_boomers', 'customer_demo_age_gen_gen_x', 'customer_demo_geography_1_3', 'customer_demo_geography_1_8']]\n",
        "df_rfe.info()"
      ],
      "metadata": {
        "id": "r3pv9uhmsKhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Model"
      ],
      "metadata": {
        "id": "g8Paoh4TsX2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have tried other models (like Random Forest and Gradient Boost). However, they are more expensive and don't allow feature importance to extracted in a way that easy to understand and operationalize. You will see below that I have taken care to address a primary concern with Decision Trees; Over fitting."
      ],
      "metadata": {
        "id": "SQ28BpF8DRg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data set for DTC algorithm"
      ],
      "metadata": {
        "id": "WarsHx0x1yvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Decision Tree Classifier\n",
        "\n",
        "df_dtc = df_rfe.copy()"
      ],
      "metadata": {
        "id": "RxOeNsYVnzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAmA2I31Wur_"
      },
      "outputs": [],
      "source": [
        "#Create Decision Tree classifer object\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dtc = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQsYhVVmisnM"
      },
      "outputs": [],
      "source": [
        "# Address class imbalance of the target\n",
        "\n",
        "df_dtc.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBeXnqm1isp7"
      },
      "outputs": [],
      "source": [
        "# Oversample the minority class using \"random over sampling\"\n",
        "# This linear approach supports cleaner interpetation of\n",
        "# feature importance.\n",
        "\n",
        "count_class_0, count_class_1 = df_dtc.target.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_class_0 = df_dtc[df_dtc['target'] == 0] #majority class\n",
        "df_class_1 = df_dtc[df_dtc['target'] == 1] #minority class\n",
        "\n",
        "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_over.target.value_counts())\n",
        "\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYkaFL5NissU"
      },
      "outputs": [],
      "source": [
        "# Shuffle the array before train/test split\n",
        "\n",
        "df_over = shuffle(df_over,random_state=0)\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test splits\n",
        "\n",
        "X, y = df_over, df_over.pop(\"target\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "50L2tHwi8AWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkWOesYvWusA"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "model_dtc.fit(X_train, y_train.values)\n",
        "score_train = model_dtc.score(X_train, y_train)\n",
        "print(\"score_train: \", score_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zuFXJK3pM6u"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "y_pred = model_dtc.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results above are reasonable, but may be due to overfitting."
      ],
      "metadata": {
        "id": "efSxzYSSEk9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the appropriate value to plot train vs test to indenitfy potential ofverfitting.\n",
        "\n",
        "**Accuracy**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Simplicity: Accuracy is straightforward to understand and interpret. It's the percentage of correctly classified instances out of all instances.\n",
        "Usefulness in Balanced Datasets: In cases where classes are balanced, accuracy can be a reliable measure of model performance.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Misleading in Imbalanced Datasets: In situations where there is a significant class imbalance, accuracy can be misleading. A model could predict the majority class for all instances and still achieve high accuracy.\n",
        "No Insight into Type I/II Errors: Accuracy doesn't distinguish between the types of errors (false positives and false negatives).\n",
        "<br>\n",
        "<br>\n",
        "**F1-Score**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Balance Between Precision and Recall: F1-score provides a balance between precision and recall. It is especially useful in cases where we need to balance false positives and false negatives.\n",
        "Better for Imbalanced Datasets: It is more informative than accuracy in case of an imbalanced dataset.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "More Complex to Understand: F1-score is not as intuitive as accuracy, especially for non-technical stakeholders.\n",
        "Not a Single Error Type Focus: If your specific problem requires optimizing specifically for either precision or recall (but not both), F1-score might not be the best metric.\n",
        "<br>\n",
        "<br>\n",
        "**ROC-AUC**\n",
        "<br>\n",
        "<br>\n",
        "Pros:\n",
        "<br>\n",
        "Performance Across Thresholds: AUC-ROC measures the model's performance across all classification thresholds, providing a comprehensive view of its effectiveness.\n",
        "Useful for Imbalanced Datasets: Like F1-score, it is more informative than accuracy for imbalanced classes.\n",
        "<br>\n",
        "<br>\n",
        "Cons:\n",
        "<br>\n",
        "Can Be Overly Optimistic: In highly imbalanced datasets, ROC-AUC might present an overly optimistic view of the model’s performance.\n",
        "Complexity in Interpretation: Understanding and explaining ROC curves and AUC can be more complex compared to straightforward metrics like accuracy."
      ],
      "metadata": {
        "id": "H7y0u4XpJs-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoMHDJHHpM9I"
      },
      "outputs": [],
      "source": [
        "# Look for overfitting via Accuracy\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 15)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "\n",
        "  # configure the model\n",
        "  model_dtc = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # fit model on the training dataset\n",
        "  model_dtc.fit(X_train, y_train)\n",
        "\n",
        "  # evaluate on the train dataset\n",
        "  train_yhat = model_dtc.predict(X_train)\n",
        "  train_acc = accuracy_score(y_train, train_yhat)\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  test_yhat = model_dtc.predict(X_test)\n",
        "  test_acc = accuracy_score(y_test, test_yhat)\n",
        "  test_scores.append(test_acc)\n",
        "\n",
        "# summarize progress\n",
        "  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look for overfitting via F1-score\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 15)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Configure the model\n",
        "    model_dtc = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # Fit model on the training dataset\n",
        "    model_dtc.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = model_dtc.predict(X_train)\n",
        "    train_f1 = f1_score(y_train, train_yhat, average='binary')  # Modify for binary/multiclass\n",
        "    train_scores.append(train_f1)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = model_dtc.predict(X_test)\n",
        "    test_f1 = f1_score(y_test, test_yhat, average='binary')  # Modify for binary/multiclass\n",
        "    test_scores.append(test_f1)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('>%d, train: %.3f, test: %.3f' % (i, train_f1, test_f1))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GXY240ONLPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lower splits show some strage behavior where the Test set is out  preforming the Tran set."
      ],
      "metadata": {
        "id": "oWqe4l-7E1my"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian-Optimization Library:\n",
        "\n",
        "Pro: Offers a probabilistic model that can efficiently find the optimal parameters, especially useful when the number of experiments is limited.\n",
        "Con: Might be less efficient in high-dimensional space and requires careful choice of the prior.\n",
        "Scikit-Optimize:\n",
        "\n",
        "Pro: Integrates seamlessly with scikit-learn and offers several methods including Bayesian optimization, which is useful for optimizing expensive-to-evaluate functions.\n",
        "Con: Limited to optimization tasks and might not be as scalable as some other libraries.\n",
        "GPyOpt:\n",
        "\n",
        "Pro: Built on Gaussian Process models, it is excellent for fine-tuning where evaluations of the function are expensive.\n",
        "Con: Can be slower and less practical for large-scale hyperparameter optimization due to the computational cost of Gaussian Processes.\n",
        "Hyperopt:\n",
        "\n",
        "Pro: Uses Bayesian optimization and supports parallelization, making it efficient for large searches.\n",
        "Con: Can be complex to configure and understand, especially for beginners.\n",
        "SHERPA:\n",
        "\n",
        "Pro: Designed for hyperparameter tuning of machine learning models, supports a variety of algorithms, and is easy to use.\n",
        "Con: Might not offer as wide a range of optimization algorithms compared to some other tools.\n",
        "Optuna:\n",
        "\n",
        "Pro: A modern library with an easy-to-use interface, it offers efficient and flexible optimization with visualization features.\n",
        "Con: Its flexibility might come at the cost of a steeper learning curve for advanced features.\n",
        "Ray Tune:\n",
        "\n",
        "Pro: Highly scalable, supports a wide range of optimization algorithms, and integrates well with deep learning frameworks.\n",
        "Con: Its broad functionality can make it more complex to set up and use.\n",
        "Neural Network Intelligence (NNI):\n",
        "\n",
        "Pro: Designed for neural networks, it offers a rich set of tuning strategies and easy integration with popular deep learning frameworks.\n",
        "Con: More focused on neural networks, might be less applicable for other types of models.\n",
        "MLMachine:\n",
        "\n",
        "Pro: Provides an easy-to-use framework for machine learning workflows, including hyperparameter tuning.\n",
        "Con: Less known and might lack some advanced features or optimizations found in more established libraries.\n",
        "Talos:\n",
        "\n",
        "Pro: Specifically designed for Keras models, making it very convenient for users of this framework.\n",
        "Con: Limited to Keras, which might not be suitable if you're using different machine learning frameworks.\n",
        "GridSearchCV:\n",
        "\n",
        "Pro: Part of scikit-learn, very straightforward and easy to use for exhaustive search over specified parameter values.\n",
        "Con: Computationally expensive as it evaluates all possible combinations and not efficient for large hyperparameter spaces."
      ],
      "metadata": {
        "id": "NxRxwA3ztaT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTGReefuou-4"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "for parameter in model.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity\n",
        "Pros:\n",
        "\n",
        "Faster Computation: Gini impurity is computationally less intensive as it doesn't involve logarithmic calculations, which can be advantageous for large datasets.\n",
        "Performance: It tends to work well in practice and is the default in many decision tree algorithms, like in scikit-learn's DecisionTreeClassifier.\n",
        "Cons:\n",
        "\n",
        "Less Sensitive to Changes in Class Probabilities: Gini impurity might be less sensitive to probability changes of the minority class, as it squares the probability terms.\n",
        "Entropy (Information Gain)\n",
        "Pros:\n",
        "\n",
        "Sensitivity to Class Probability Changes: Entropy is more sensitive to changes in the class probabilities of the nodes, potentially leading to more balanced trees.\n",
        "Information Theoretic Model: It has a basis in information theory, providing a clear interpretation in terms of information content and uncertainty.\n",
        "Cons:\n",
        "\n",
        "Computational Intensity: Calculating entropy involves logarithmic computations, which can be more computationally intensive than Gini impurity, especially for very large datasets.\n",
        "Can Lead to Overfitting: In some cases, because entropy is more sensitive to class probabilities, it can lead to models that are more complex and potentially overfitted."
      ],
      "metadata": {
        "id": "EFzSVLOzO29q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCeSue33ovBJ"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "\n",
        "param_dict = {'criterion' :['gini', 'entropy'],\n",
        "            'max_depth' :range(1,15),\n",
        "            'min_samples_split' : range(20, 50,10), # From Central Limit Theory\n",
        "            'min_samples_leaf': range(20, 50, 10),\n",
        "             'ccp_alpha' :[0.0001, 0.001, 0.01]}\n",
        "\n",
        "tree_class = DecisionTreeClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 5,\n",
        "        verbose= 1,\n",
        "        n_jobs = -1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3aAZ1g5ovET"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming tested parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKNc5NNCnsuf"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_dtc_p=DecisionTreeClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='entropy',\n",
        "                                   max_depth = 6,\n",
        "                                   min_samples_split = 20,\n",
        "                                   min_samples_leaf = 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtDlf8pCisxf"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc_p.score(X_test, y_test)\n",
        "print(\"score_train: \", score_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THUeLYtSPHKx"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "y_pred = model_dtc_p.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look again for overfitting via Accuracy\n",
        "\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 15)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "\n",
        "  # configure the model\n",
        "  model_dtc_p = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # fit model on the training dataset\n",
        "  model_dtc_p.fit(X_train, y_train)\n",
        "\n",
        "  # evaluate on the train dataset\n",
        "  train_yhat = model_dtc_p.predict(X_train)\n",
        "  train_acc = accuracy_score(y_train, train_yhat)\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  test_yhat = model_dtc_p.predict(X_test)\n",
        "  test_acc = accuracy_score(y_test, test_yhat)\n",
        "  test_scores.append(test_acc)\n",
        "\n",
        "# summarize progress\n",
        "  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q7vq_gqFN5Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the accuracey and F-1 based curves show an issue in the lower branches."
      ],
      "metadata": {
        "id": "Kn1s2cTYPzJE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9oWbgjpvrM6"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_dtc_p.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Global SHAP values\n",
        "\n",
        "exp = TreeExplainer(model_dtc)\n",
        "sv = exp.shap_values(X_test)\n",
        "\n",
        "# Initialize an empty list to store feature statistics\n",
        "feature_stats = []\n",
        "feature_to_drop = []\n",
        "\n",
        "# Loop through each feature and calculate min, mean, and max SHAP values\n",
        "for i in range(len(X_test.columns)):\n",
        "    feature_name = X_test.columns[i]\n",
        "    shap_values_feature = sv[1][:, i]\n",
        "    min_val = np.min(shap_values_feature)\n",
        "    mean_val = np.mean(shap_values_feature)\n",
        "    max_val = np.max(shap_values_feature)\n",
        "    feature_stats.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "\n",
        "    # Append the statistics to the list of features to drop\n",
        "    # if 1==1:#not (min_val == mean_val == max_val == 0):\n",
        "    if (min_val == mean_val == max_val == 0):\n",
        "        feature_to_drop.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "# Create a DataFrames of all feature with their min, mean, max values\n",
        "feature_stats_df = pd.DataFrame(feature_stats, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_stats.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_stats_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Create a DataFrames of dropped features\n",
        "feature_to_drop_df = pd.DataFrame(feature_to_drop, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_to_drop_df.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_to_drop_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Plots\n",
        "# Display the SHAP summary plot for all features\n",
        "shap.summary_plot(sv[1], X_test, max_display=20)\n",
        "\n",
        "# Plotting the mean SHAP values for each feature\n",
        "plt.figure(figsize=(10, len(feature_stats_df) / 2))  # Adjust the figure size as needed\n",
        "feature_stats_df.sort_values(by='Mean SHAP Value', ascending=True, inplace=True)\n",
        "plt.barh(feature_stats_df['Feature'], feature_stats_df['Mean SHAP Value'], color='skyblue')\n",
        "plt.xlabel('Mean SHAP Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Mean SHAP Values for Each Feature')\n",
        "plt.show()\n",
        "\n",
        "print(feature_stats_df)\n"
      ],
      "metadata": {
        "id": "ihM8GljGUw-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Force Plot visualization: single customer (Local SHAP values)\n",
        "\n",
        "# Calculate SHAP values for the first row of the X_test dataset\n",
        "shap_values_single = exp.shap_values(X_test.iloc[120]) # Good Exxample of really bad churn\n",
        "\n",
        "# Initialize JavaScript visualization in Google Colab (even if it might not display)\n",
        "shap.initjs()\n",
        "\n",
        "# Generate a force plot using matplotlib backend\n",
        "plt.figure(figsize=(40, 10))  # You can adjust the size as needed\n",
        "shap.force_plot(exp.expected_value[1], shap_values_single[1], X_test.iloc[0], feature_names=X_test.columns, matplotlib=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6_561r2wDRb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter customers based on all specified features and values simultaneously\n",
        "\n",
        "high_value_features = {\n",
        "    \"product_cpe_6\": 1,\n",
        "    \"product_sku_2\": 1\n",
        "    #\"sales_team_3\": 1\n",
        "    #\"customer_demo_age_gen_boomers\" : 1\n",
        "    #\"sales_program_18\" : 1\n",
        "                       }\n",
        "\n",
        "filtered_customers = X_test.copy()\n",
        "\n",
        "for feature, value in high_value_features.items():\n",
        "    filtered_customers = filtered_customers[filtered_customers[feature] == value]\n",
        "\n",
        "# DataFrame to store SHAP values\n",
        "shap_values_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over filtered customers to calculate SHAP values and generate force plots\n",
        "for index, row in filtered_customers.iterrows():\n",
        "    # Calculate SHAP values for the current example\n",
        "    shap_values_example = exp.shap_values(row)\n",
        "\n",
        "    # Append the SHAP values to the DataFrame\n",
        "    shap_row_df = pd.DataFrame([shap_values_example[1]], columns=[f'shap_{col}' for col in X_test.columns])\n",
        "    shap_values_df = pd.concat([shap_values_df, shap_row_df], ignore_index=True)\n",
        "\n",
        "    # Generate a force plot for the example\n",
        "    shap.initjs()\n",
        "    shap.force_plot(exp.expected_value[1], shap_values_example[1], row, matplotlib=True)\n",
        "    plt.show()\n",
        "\n",
        "# Concatenate the SHAP values DataFrame with the filtered customers DataFrame\n",
        "final_df = pd.concat([filtered_customers.reset_index(drop=True), shap_values_df], axis=1)\n",
        "\n",
        "# Export to CSV\n",
        "final_df.to_csv('shap_values.csv', index=False)"
      ],
      "metadata": {
        "id": "n4eHaz2XkhfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boost\n"
      ],
      "metadata": {
        "id": "XEmHEooTsei6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Random Forest Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "df_gb = df_pre.copy()"
      ],
      "metadata": {
        "id": "__72wYDms10Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Address class imbalance of the target\n",
        "\n",
        "df_gb.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7DSuhz_btVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjyIfLO_tytv"
      },
      "outputs": [],
      "source": [
        "# Oversample the minority class using \"random over sampling\"\n",
        "# This linear approach supports cleaner interpetation of\n",
        "# feature importance with SHAP.\n",
        "\n",
        "count_class_0, count_class_1 = df_dtc.target.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_class_0 = df_gb[df_gb['target'] == 0] #majority class\n",
        "df_class_1 = df_gb[df_gb['target'] == 1] #minority class\n",
        "\n",
        "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_over.target.value_counts())\n",
        "\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vNW8seVtyt7"
      },
      "outputs": [],
      "source": [
        "# Shuffle the array before train/test split\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_over = shuffle(df_over,random_state=0)\n",
        "df_over.target.value_counts().plot(kind='bar', title='Count (target)');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test splits\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = df_over, df_over.pop(\"target\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Small test size only to support feature reduction via SHAP"
      ],
      "metadata": {
        "id": "u4E0LUI2tyt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "\n",
        "# Fit the model to the training data\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Kd6Y1GRas123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEcpXoR4ulte"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score\n",
        "\n",
        "y_pred = gb_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGxo-I-Zultf"
      },
      "outputs": [],
      "source": [
        "# Look for any overfitting of the GB model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 10)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "    # Initialize the Gradient Boosting Classifier with varying max_depth\n",
        "    gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=i, random_state=0)\n",
        "\n",
        "    # Fit the model to the training data\n",
        "    gb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on the train dataset\n",
        "    train_yhat = gb_model.predict(X_train)\n",
        "    train_acc = accuracy_score(y_train, train_yhat)\n",
        "    train_scores.append(train_acc)\n",
        "\n",
        "    # Evaluate on the test dataset\n",
        "    test_yhat = gb_model.predict(X_test)\n",
        "    test_acc = accuracy_score(y_test, test_yhat)\n",
        "    test_scores.append(test_acc)\n",
        "\n",
        "    # Summarize progress\n",
        "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# Plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1mOsNLZultf"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "for parameter in model.get_params():\n",
        "    print(parameter)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILTN-jf9ultf"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_dict = {\"criterion\" :[\"friedman_mse\", \"squared_error\"],\n",
        "            \"max_depth\" :range(1,10),\n",
        "            #\"min_samples_split\" :range(2,10),\n",
        "            #\"min_samples_leaf\" :range(1,5),\n",
        "             \"ccp_alpha\" :[0.0001, 0.001, 0.01]}\n",
        "\n",
        "tree_class = GradientBoostingClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 10,\n",
        "        verbose= 1,\n",
        "        n_jobs = 1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQYsRE9Rultf"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming tested parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLr1D6Xjultf"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_dtc_p=DecisionTreeClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='entropy',\n",
        "                                   max_depth = 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbXt6Zl1ultg"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc_p.score(X_test, y_test)\n",
        "print(\"score_train: \", score_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQYdYbIAultg"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model_dtc_p.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItYlmIU-ultg"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "model_gb = GradientBoostingClassifier()\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# fit a model\n",
        "#model = DecisionTreeClassifier()\n",
        "model_gb.fit(X_train, y_train.values)\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_dtc_p.predict_proba(X_test)\n",
        "\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "# Train a decision tree model\n",
        "model = DecisionTreeClassifier(max_depth=3)  # Limit depth for visualization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Export as dot file\n",
        "dot_data = export_graphviz(model, out_file=None,\n",
        "                           feature_names=X_train.columns,\n",
        "                           class_names=['0', '1'],  # Assuming binary classification with classes '0' and '1'\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                           proportion=False,  # Set to 'True' to show percentages instead of sample counts\n",
        "                           precision=2,  # Set precision for floating point numbers\n",
        "                           label='all',  # Use 'root' to show labels at the root node or 'all' to show at all nodes\n",
        "                           leaves_parallel=False)  # Set to 'True' to align leaf nodes horizontally\n",
        "\n",
        "# Create graph from dot data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Render and show the graph\n",
        "Image(graph.pipe(format='png'))\n"
      ],
      "metadata": {
        "id": "Oo_032UpQhT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance from the Gradient model\n",
        "\n",
        "feature_importances = model_gb.feature_importances_\n",
        "\n",
        "# To display feature importance\n",
        "for i, feature in enumerate(X_train.columns):\n",
        "    print(f'Feature: {feature}, Importance: {feature_importances[i]}')"
      ],
      "metadata": {
        "id": "Q5GvheCOs192"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnlwKttCs2AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4f7uG9rs2Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9UeFVVvps2FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzqSpWARs2Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WShEXN3Is2J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIf4D98Us2MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_mmjfEp6s2Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a50ZOowAs2RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1aoTVFNs2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What follows is another full run but w/o over sample the minority class"
      ],
      "metadata": {
        "id": "Ot4y44hcnY2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Results were not usable*"
      ],
      "metadata": {
        "id": "OaO7U4Jy0hzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data set for DTC algorithm"
      ],
      "metadata": {
        "id": "vlBFSkCYxFqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parking the df at this point to allow for additional experiments\n",
        "\n",
        "# For decision tree\n",
        "df_dtc_2 = df_pre.copy()\n"
      ],
      "metadata": {
        "id": "O_0e_JuUxOuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoQthqHbxFqh"
      },
      "outputs": [],
      "source": [
        "# Address class imbalance of the target\n",
        "\n",
        "df_dtc_2.target.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wy-PlP6xFqh"
      },
      "outputs": [],
      "source": [
        "# Shuffle the array before train/test split\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_dtc_2 = shuffle(df_dtc_2,random_state=0)\n",
        "df_dtc_2.target.value_counts().plot(kind='bar', title='Count (target)');"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/test splits\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = df_dtc_2, df_dtc_2.pop(\"target\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Small test size only to support feature reduction via SHAP"
      ],
      "metadata": {
        "id": "MOMPfhMVxFqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Wc7sTVxFqi"
      },
      "outputs": [],
      "source": [
        "#Create Decision Tree classifer object\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dtc_2 = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeQACxPuxFqi"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "model_dtc_2.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc_2.score(X_train, y_train)\n",
        "print(\"score_train: \", score_train)\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuwSxUw4xFqi"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score\n",
        "\n",
        "y_pred = model_dtc_2.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bYcR27GxFqi"
      },
      "outputs": [],
      "source": [
        "# Look for any overfitting of the DTC model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the tree depths to evaluate\n",
        "values = [i for i in range(1, 15)]\n",
        "\n",
        "# Define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "\n",
        "# Evaluate for each depth\n",
        "for i in values:\n",
        "\n",
        "  # configure the model\n",
        "  model_dtc = DecisionTreeClassifier(max_depth=i)\n",
        "\n",
        "    # fit model on the training dataset\n",
        "  model_dtc_2.fit(X_train, y_train)\n",
        "\n",
        "  # evaluate on the train dataset\n",
        "  train_yhat = model_dtc_2.predict(X_train)\n",
        "  train_acc = accuracy_score(y_train, train_yhat)\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # evaluate on the test dataset\n",
        "  test_yhat = model_dtc_2.predict(X_test)\n",
        "  test_acc = accuracy_score(y_test, test_yhat)\n",
        "  test_scores.append(test_acc)\n",
        "\n",
        "# summarize progress\n",
        "  print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk0hXYz9xFqi"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "for parameter in model.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg4o1WcOxFqi"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to tune the model\n",
        "# and resolve any overfitting\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_dict = {\"criterion\" :[\"gini\", \"entropy\"],\n",
        "            \"max_depth\" :range(1,10),\n",
        "            #\"min_samples_split\" :range(2,10),\n",
        "            #\"min_samples_leaf\" :range(1,5),\n",
        "             \"ccp_alpha\" :[0.0001, 0.001, 0.01]}\n",
        "\n",
        "tree_class = DecisionTreeClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 10,\n",
        "        verbose= 1,\n",
        "        n_jobs = 1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEwm15nQxFqi"
      },
      "outputs": [],
      "source": [
        "# Extact the best preforming tested parameter values\n",
        "\n",
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izpzgj5rxFqi"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_dtc_p=DecisionTreeClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='entropy',\n",
        "                                   max_depth = 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzlw2V6_xFqi"
      },
      "outputs": [],
      "source": [
        "#Fit and re-score the model\n",
        "\n",
        "model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc_p.score(X_test, y_test)\n",
        "print(\"score_train: \", score_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83w9aWkwxFqi"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = model_dtc_p.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfozUEmNxFqj"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# fit a model\n",
        "#model = DecisionTreeClassifier()\n",
        "#model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_dtc_p.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare enviornment for feature importance/reduction\n",
        "# https://towardsdatascience.com/demystify-your-ml-model-with-shap-fc191a1cb08a\n",
        "\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "WnpO8RutxFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap as shap\n",
        "from shap import TreeExplainer, summary_plot\n",
        "\n",
        "exp = TreeExplainer(model_dtc)\n",
        "sv = exp.shap_values(X_test)\n",
        "\n",
        "# Initialize an empty list to store feature statistics\n",
        "feature_stats = []\n",
        "feature_to_drop = []\n",
        "\n",
        "# Loop through each feature and calculate min, mean, and max SHAP values\n",
        "for i in range(len(X_test.columns)):\n",
        "    feature_name = X_test.columns[i]\n",
        "    shap_values_feature = sv[1][:, i]\n",
        "    min_val = np.min(shap_values_feature)\n",
        "    mean_val = np.mean(shap_values_feature)\n",
        "    max_val = np.max(shap_values_feature)\n",
        "    feature_stats.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "\n",
        "    # Append the statistics to the list of features to drop\n",
        "    # if 1==1:#not (min_val == mean_val == max_val == 0):\n",
        "    if (min_val == mean_val == max_val == 0):\n",
        "        feature_to_drop.append([feature_name, min_val, mean_val, max_val]) # capture which features were dropped\n",
        "\n",
        "# Create a DataFrames of all feature with their min, mean, max values\n",
        "feature_stats_df = pd.DataFrame(feature_stats, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_stats.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_stats_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Create a DataFrames of dropped features\n",
        "feature_to_drop_df = pd.DataFrame(feature_to_drop, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Load CSV file to the specified path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/sklearn/feature_to_drop_df.csv'\n",
        "\n",
        "# Save the DataFrame to the specified path as a CSV file\n",
        "feature_to_drop_df.to_csv(csv_path, index=False)  # Set index=False to avoid saving the index column\n",
        "\n",
        "\n",
        "# Plots\n",
        "# Display the SHAP summary plot for all features\n",
        "shap.summary_plot(sv[1], X_test, max_display=20)\n",
        "\n",
        "# Plotting the mean SHAP values for each feature\n",
        "plt.figure(figsize=(10, len(feature_stats_df) / 2))  # Adjust the figure size as needed\n",
        "feature_stats_df.sort_values(by='Mean SHAP Value', ascending=True, inplace=True)\n",
        "plt.barh(feature_stats_df['Feature'], feature_stats_df['Mean SHAP Value'], color='skyblue')\n",
        "plt.xlabel('Mean SHAP Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Mean SHAP Values for Each Feature')\n",
        "plt.show()\n",
        "\n",
        "print(feature_stats_df)\n"
      ],
      "metadata": {
        "id": "5Rtg6Mh0xFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Force Plot visualization: single customer (Local SHAP values)\n",
        "\n",
        "\n",
        "# Calculate SHAP values for the first row of the X_test dataset\n",
        "shap_values_single = exp.shap_values(X_test.iloc[101])\n",
        "\n",
        "# Initialize JavaScript visualization in Google Colab (even if it might not display)\n",
        "shap.initjs()\n",
        "\n",
        "# Generate a force plot using matplotlib backend\n",
        "plt.figure(figsize=(40, 10))  # You can adjust the size as needed\n",
        "shap.force_plot(exp.expected_value[1], shap_values_single[1], X_test.iloc[0], feature_names=X_test.columns, matplotlib=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AYl_SeYSxFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually specify the top features and their values for filtering\n",
        "high_value_features = {\"product_sku_2\": 1,\n",
        "                        \"product_cpe_6\": 1,\n",
        "                        \"sales_team_17\": 1,\n",
        "                        \"sales_program_13\": 1\n",
        "                       }\n",
        "# Create a DataFrame to store filtered customers\n",
        "filtered_customers = pd.DataFrame()\n",
        "\n",
        "# Iterate over specified features and values\n",
        "for feature, value in high_value_features.items():\n",
        "    # Filter customers based on the specified feature and value\n",
        "    filtered_customers = pd.concat([filtered_customers, X_test[X_test[feature] == value]])\n",
        "\n",
        "# Now, filtered_customers contains only the customers that match the specified features and values\n",
        "\n",
        "# Iterate over filtered customers to calculate and print force plots\n",
        "for index, row in filtered_customers.iterrows():\n",
        "    # Calculate SHAP values for the current example\n",
        "    shap_values_example = exp.shap_values(row)\n",
        "\n",
        "    # Generate a force plot for the example\n",
        "    shap.initjs()  # Initialize JS visualization (if not already done)\n",
        "    shap.force_plot(exp.expected_value[1], shap_values_example[1], row, matplotlib=True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "seNpnjitxFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eHOaZuznfv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwfSnOfmnfyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cd7vyRKInf0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLEfU8SBnf3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4PgBPVuInf5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vQdy3_Fnf74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyLe2pH2nf-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Evyp7KTFngAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eg0xo7L8ngCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WxtEvySngE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TP8ZuoRlngHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-2-aeZ5ngJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_WjQdringL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CVyOn9kUngOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LorHyZqngQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HpUk0l1ngTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix / Parking lot"
      ],
      "metadata": {
        "id": "EmJDF_uI1Rkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from shap import TreeExplainer\n",
        "\n",
        "\n",
        "# Create the SHAP explainer and calculate SHAP values\n",
        "exp = TreeExplainer(model_dtc)\n",
        "sv = exp.shap_values(X_test)\n",
        "\n",
        "# Initialize an empty list for features to drop\n",
        "feature_to_drop = []\n",
        "\n",
        "# Loop through each feature and calculate min, mean, and max SHAP values\n",
        "for i in range(len(X_test.columns)):\n",
        "    feature_name = X_test.columns[i]\n",
        "    shap_values_feature = sv[1][:, i]\n",
        "    # min_val = np.min(shap_values_feature)\n",
        "    # max_val = np.max(shap_values_feature)\n",
        "    mean_val = np.mean(shap_values_feature)\n",
        "\n",
        "\n",
        "    # Append the feature to the list\n",
        "    #if (min_val == mean_val == max_val == 0):\n",
        "    if (mean_val == 0):\n",
        "        feature_to_drop.append(feature_name)\n",
        "\n",
        "# Print the list of features to drop\n",
        "print(\"Features to drop based on SHAP values:\", feature_to_drop)\n",
        "\n"
      ],
      "metadata": {
        "id": "fPvIG31NAWeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Your pandas operation that causes the warning\n",
        "\n",
        "\n",
        "#from sklearn.datasets import make_classification\n",
        "\n",
        "#import xgboost as xgb\n",
        "\n",
        "#from sklearn import metrics\n",
        "#from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "#from sklearn.feature_selection import chi2, RFE\n",
        "\n",
        "#from sklearn.tree import export_graphviz\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#from sklearn.model_selection import cross_val_score\n",
        "#from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "#from sklearn.feature_selection import RFE\n",
        "#from sklearn.pipeline import Pipeline\n",
        "#from sagemaker import get_execution_role\n",
        "#from sklearn.externals.six import StringIO\n",
        "#from sklearn.tree import export_graphviz\n",
        "#from sklearn.model_selection import validation_curve\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#from sklearn import metrics\n",
        "#from sklearn import tree\n",
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "#from sklearn.manifold import TSNE\n",
        "\n",
        "#from sklearn.svm import LinearSVC, SVC\n",
        "#from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "#!pip install flake8 pycodestyle_magic\n",
        "\n",
        "# Needed for Sagemaker\n",
        "#!pip install botocore\n",
        "\n",
        "# Needed for Sagemaker\n",
        "#pip install s3fs==2021.08.0\n",
        "\n",
        "#from numpy import mean\n",
        "#from numpy import std"
      ],
      "metadata": {
        "id": "8KlEqgy7GOul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming sv contains the SHAP values computed earlier\n",
        "# and X_test is your test dataset\n",
        "\n",
        "# Initialize an empty list to store feature statistics\n",
        "feature_stats = []\n",
        "\n",
        "# Loop through each feature and calculate min, mean, and max SHAP values\n",
        "for i in range(len(X_test.columns)):\n",
        "    feature_name = X_test.columns[i]\n",
        "    shap_values_feature = sv[1][:, i]\n",
        "    min_val = np.min(shap_values_feature)\n",
        "    mean_val = np.mean(shap_values_feature)\n",
        "    max_val = np.max(shap_values_feature)\n",
        "\n",
        "    # Append the statistics to the list if they are not all zero\n",
        "    if not (min_val == mean_val == max_val == 0):\n",
        "        feature_stats.append([feature_name, min_val, mean_val, max_val])\n",
        "\n",
        "# Create a DataFrame from the list\n",
        "feature_stats_df = pd.DataFrame(feature_stats, columns=['Feature', 'Min SHAP Value', 'Mean SHAP Value', 'Max SHAP Value'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(feature_stats_df)\n",
        "\n",
        "# Display the SHAP summary plot for all features\n",
        "shap.summary_plot(sv[1], X_test, max_display=100)\n"
      ],
      "metadata": {
        "id": "hZ3cSx73CmEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSFLhTgdSFPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7IFOeODSFSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SeMy8RsSFVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkzTAJHwSFX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NOPsv0AqSFag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUmKiOEkSFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZcHSIv9SFf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iaa7RXJFSFis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "\n",
        "# Assuming exp is your pre-initialized SHAP explainer and other necessary imports are done\n",
        "custom_baseline = 0.50  # Your custom baseline value\n",
        "\n",
        "# Iterate over filtered customers to calculate and print force plots\n",
        "for index, row in filtered_customers.iterrows():\n",
        "    # Calculate SHAP values for the current example\n",
        "    shap_values_example = exp.shap_values(row)\n",
        "\n",
        "    # Manually adjust the expected value for the force plot\n",
        "    adjusted_expected_value = custom_baseline\n",
        "\n",
        "    # Generate a force plot for the example with the adjusted expected value\n",
        "    shap.initjs()  # Initialize JS visualization (if not already done)\n",
        "    shap.force_plot(adjusted_expected_value, shap_values_example[1], row, matplotlib=True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CAXclvqHzVhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance matrix\n",
        "\n",
        "from shap import TreeExplainer, summary_plot\n",
        "\n",
        "model_dtc_p = model_dtc_p.fit(X_train, y_train)\n",
        "\n",
        "exp = TreeExplainer(model_dtc_p)\n",
        "sv = exp.shap_values(X_test)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = X_test.columns\n",
        "\n",
        "# Get the feature importance values for the chosen row\n",
        "feature_importance_values = sv[1][0]\n",
        "\n",
        "# Ensure that the lengths match\n",
        "if len(feature_names) == len(feature_importance_values):\n",
        "    # Create a DataFrame with feature names and their importance values\n",
        "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance_values})\n",
        "\n",
        "    # Print the DataFrame\n",
        "    #print(\"Feature Importance DataFrame:\")\n",
        "    #print(feature_importance_df)\n",
        "\n",
        "\n",
        "# Sort the DataFrame by the importance column in ascending order\n",
        "sorted_df = feature_importance_df.sort_values(by='Importance')\n",
        "\n",
        "# Iterate over the sorted DataFrame and print feature and importance\n",
        "for Feature, Importance in sorted_df.itertuples(index=False):\n",
        "    print(f'Feature: {Feature}, Importance: {Importance}')"
      ],
      "metadata": {
        "id": "gsuvREno-c1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the models base value ?\n",
        "\n",
        "# Assuming model_dtc_p is your decision tree classifier and is already fitted with X_train and y_train\n",
        "# model_dtc_p = model_dtc_p.fit(X_train, y_train)\n",
        "\n",
        "# Initialize the TreeExplainer with your fitted model\n",
        "exp = TreeExplainer(model_dtc_p)\n",
        "\n",
        "# Print the expected values\n",
        "print(\"Expected Value for Class 0:\", exp.expected_value[0])\n",
        "print(\"Expected Value for Class 1:\", exp.expected_value[1])"
      ],
      "metadata": {
        "id": "UlDPsyyJQzCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYjdOy-ovrPe"
      },
      "outputs": [],
      "source": [
        "# Feature importance visualization: Global SHAP values\n",
        "\n",
        "import shap as shap\n",
        "from shap import TreeExplainer, summary_plot\n",
        "\n",
        "model_dtc_p = model_dtc_p.fit(X_train, y_train)\n",
        "\n",
        "exp = TreeExplainer(model_dtc_p)\n",
        "sv = exp.shap_values(X_test)\n",
        "summary_plot(sv[1], X_test, max_display=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae4prKpOisu-"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing dataset.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(df,train_size=0.66)\n",
        "display(train_data)\n",
        "display(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6sBCddqnsfg"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "X_train = train_data.iloc[:,1:]\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5u8FYhbnso7"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "y_train = train_data.iloc[:, [0]]\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Q02PQWnsrl"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "X_test = test_data.iloc[:,1:]\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzuEw7lpou5H"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "y_test = test_data.iloc[:, [0]]\n",
        "y_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "heart_df = df_over.copy()\n",
        "\n",
        "X, y = heart_df, heart_df.pop(\"target\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "aV2Ev9-lybth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collinearity: PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the data\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X_independent)  # X_independent contains your numeric features\n",
        "\n",
        "X_scaled = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "\n",
        "# Apply PCA with the desired number of components\n",
        "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# X_pca contains the transformed features\n",
        "print(\"Explained Variance Ratio:\")\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "kcy7XRQNcj5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate RFE for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# create pipeline\n",
        "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
        "model = DecisionTreeClassifier()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "id": "u-uZp0wTkMcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l50xtOiC-c_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9F6YXW7H-dCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g2kI0o_vrUP"
      },
      "outputs": [],
      "source": [
        "# Train/test splits\n",
        "\n",
        "X, y = df, df.pop(\"target\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "KaVaolRwqulc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "wsLYTMXdquoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Assuming you have already computed shap_values and the expected value\n",
        "# Replace 'expected_value' with the actual expected value for your model\n",
        "expected_value = 0.0  # Replace with your model's expected value\n",
        "\n",
        "# Plot the SHAP values for a specific data point (e.g., index 100)\n",
        "shap.plots.force(expected_value, shap_values[100])\n"
      ],
      "metadata": {
        "id": "hBn0dswvquqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyYST-XbquvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPJI5kH0quxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x44wV_4Iqu0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gu91p8LVqu2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P_Bgu3m4qu46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Z--NVHaqu7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mw8wdYrkqu96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nvliYPSqvAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bMgi_Yzis0F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_scNtJYis2T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlijUo-6dAGn"
      },
      "outputs": [],
      "source": [
        "# Bin/classify of fulfillment_time\n",
        "\n",
        "df['fulfillment'] = df['fulfillment_time'].apply(lambda fulfillment_time:\n",
        "    1 if fulfillment_time == 1 else 0\n",
        ")\n",
        "\n",
        "# Drop fulfillment_time\n",
        "df.drop('fulfillment_time', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFf6xe2MibWi"
      },
      "source": [
        "### Original model. I dont have the source file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRkQ22cw4b7B"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv(r'/content/qp_model_202301062016.csv')\n",
        "print(df_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGUZ6YtEBOWz"
      },
      "outputs": [],
      "source": [
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyNsE1Bul8Xl"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_raw.info()\n",
        "\n",
        "#<class 'pandas.core.frame.DataFrame'>\n",
        "##RangeIndex: 265913 entries, 0 to 265912\n",
        "#d ata columns (total 17 columns):\n",
        "# Column                           Non-Null Count   Dtype\n",
        "-#--  ------                           --------------   -----\n",
        "# 0   churn_flag                       265913 non-null  int64\n",
        "# 1   age                              265913 non-null  int64\n",
        "# 2   data_points                      265913 non-null  int64\n",
        "# 3   voice_points                     265913 non-null  int64\n",
        "# 4   sms_points                       265913 non-null  int64\n",
        "# 5   total_points                     265913 non-null  int64\n",
        "# 6   count_of_approved_accounts_hist  265913 non-null  int64\n",
        "# 7   master                           265913 non-null  object\n",
        "# 8   state                            265913 non-null  object\n",
        "# 9   channel                          265913 non-null  object\n",
        "# 10  plantype                         265913 non-null  object\n",
        "# 11  eligibilityprogram               265913 non-null  object\n",
        "# 12  devicebrand                      265913 non-null  object\n",
        "# 13  bqplancode                       265913 non-null  int64\n",
        "# 14  is_ported                        265913 non-null  int64\n",
        "# 15  eligibility_new                  265913 non-null  int64\n",
        "# 16  groupchannelinperson             265913 non-null  int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9JVGONfmL7J"
      },
      "outputs": [],
      "source": [
        "# Original model. I dont have the source file.\n",
        "df_raw.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCIhojRdmZbY"
      },
      "outputs": [],
      "source": [
        "# Original model. I dont have the source file.\n",
        "df_raw.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUHAf0ozyP4A"
      },
      "source": [
        "Need to address class imbalance.<br>\n",
        "groupchannelinperson is almost certainly wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSQdUUSn7o4t"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Look for Null values\n",
        "df_raw.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXp4QL0P72T0"
      },
      "outputs": [],
      "source": [
        "\n",
        "nan_count = df_raw.isna().sum()\n",
        "print(nan_count )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL4QtzFYCcU0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_raw = df_raw.drop (['total_points','count_of_approved_accounts_hist','master','state','channel',\n",
        "                       'eligibilityprogram','devicebrand','bqplancode','is_ported','groupchannelinperson'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pi4hpIRBL_8"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0HoUwvhtIHq"
      },
      "outputs": [],
      "source": [
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-PdIUVNB010"
      },
      "outputs": [],
      "source": [
        "# age is a continuous variable that will need to be normalized.\n",
        "\n",
        "column = 'age'\n",
        "\n",
        "df_raw[column] = df_raw[column] /df_raw[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXJ_jgB9CDUp"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'data_points'\n",
        "\n",
        "df_raw[column] = df_raw[column] /df_raw[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQb-lXYxCI7e"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'voice_points'\n",
        "\n",
        "df_raw[column] = df_raw[column] /df_raw[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybaf94SlCKyT"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'sms_points'\n",
        "\n",
        "df_raw[column] = df_raw[column] /df_raw[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YHq4W-bBqYy"
      },
      "outputs": [],
      "source": [
        "# Flatten the categorical features (where datatype = Object)\n",
        "\n",
        "df_raw = pd.get_dummies(df_raw, columns=['plantype'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FDvAj0ICvDr"
      },
      "outputs": [],
      "source": [
        "# Collinearity matrix of non-categorical data\n",
        "\n",
        "df_num = df_raw.select_dtypes(include = ['float64', 'int64'])\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "corrMatrix = df_num.corr()\n",
        "sns.heatmap(corrMatrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w6vVbD9ylAX"
      },
      "source": [
        "Looks clean outside of the usage fields. Might combine them in an engineered feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4A60D7TCXPK"
      },
      "outputs": [],
      "source": [
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zqk3kehBQC9"
      },
      "outputs": [],
      "source": [
        "# Checking for class imbalance\n",
        "\n",
        "#df_raw.hist(bins=50, xlabelsize=8, ylabelsize=8, column='target')\n",
        "\n",
        "df_raw.churn_flag.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkyH4JBxCrrc"
      },
      "outputs": [],
      "source": [
        "# Oversample the minority class. This needs to be done after flattening the df to maintain oversampled values.\n",
        "\n",
        "count_class_0, count_class_1 = df_raw.churn_flag.value_counts()\n",
        "# Divide by class\n",
        "df_class_0 = df_raw[df_raw['churn_flag'] == 0] #majority class\n",
        "df_class_1 = df_raw[df_raw['churn_flag'] == 1] #minority class\n",
        "\n",
        "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
        "df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_over.churn_flag.value_counts())\n",
        "\n",
        "df_over.churn_flag.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpG12BbLCw5r"
      },
      "outputs": [],
      "source": [
        "# Shuffle the array before train, test split\n",
        "\n",
        "df_over = shuffle(df_over,random_state=0)\n",
        "df_over.churn_flag.value_counts().plot(kind='bar', title='Count (target)');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E-tVjyJC0w3"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing dataset.\n",
        "\n",
        "train_data, test_data = train_test_split(df_over,train_size=0.33)\n",
        "display(train_data)\n",
        "display(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol-EGmqIC40x"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "X_train = train_data.iloc[:,1:]\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GARmZVdC8N9"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "y_train = train_data.iloc[:, [0]]\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G78VyEaMC_HH"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "X_test = test_data.iloc[:,1:]\n",
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQzkxM4IDBda"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "y_test = test_data.iloc[:, [0]]\n",
        "y_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqe9TMiODExG"
      },
      "outputs": [],
      "source": [
        "#Create Decision Tree classifer object\n",
        "\n",
        "model_dtc = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o19KADhUDHWt"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "model_dtc.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc.score(X_train, y_train)\n",
        "print(\"score_train: \", score_train)\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9gL2cyoDKmC"
      },
      "outputs": [],
      "source": [
        "# Score the test dataset\n",
        "model_dtc.fit(X_test, y_test.values.ravel())\n",
        "score_test = model_dtc.score(X_test, y_test)\n",
        "print(\"score_test: \", score_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuCqIWNRpJ-6"
      },
      "outputs": [],
      "source": [
        "y_pred = model_dtc.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ich3asAnILHu"
      },
      "source": [
        "Check for Overfitness: Learning Curves<br>\n",
        "https://machinelearningmastery.com/overfitting-machine-learning-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfPkVynAsCT7"
      },
      "outputs": [],
      "source": [
        "# define the tree depths to evaluate\n",
        "values = [i for i in range(1, 21)]\n",
        "# define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "# evaluate a decision tree for each depth\n",
        "for i in values:\n",
        " # configure the model\n",
        " model_dtc = DecisionTreeClassifier(max_depth=i)\n",
        " # fit model on the training dataset\n",
        " model_dtc.fit(X_train, y_train)\n",
        " # evaluate on the train dataset\n",
        " train_yhat = model_dtc.predict(X_train)\n",
        " train_acc = accuracy_score(y_train, train_yhat)\n",
        " train_scores.append(train_acc)\n",
        " # evaluate on the test dataset\n",
        " test_yhat = model_dtc.predict(X_test)\n",
        " test_acc = accuracy_score(y_test, test_yhat)\n",
        " test_scores.append(test_acc)\n",
        " # summarize progress\n",
        " print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        " # plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwlMZE7it7eM"
      },
      "source": [
        "Looks to be overfit after ~8 levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F6_zEKSzVrf"
      },
      "source": [
        "Might try k-fold next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytLEaHyrX4Q5"
      },
      "source": [
        "https://elitedatascience.com/overfitting-in-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONG8szIxgHrG"
      },
      "source": [
        "Tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnjrUUEQDNdl"
      },
      "outputs": [],
      "source": [
        "# Get model parameters\n",
        "\n",
        "model = DecisionTreeClassifier()\n",
        "for parameter in model.get_params():\n",
        "    print(parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-5-in4jDQfV"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameter dictionary to test\n",
        "\n",
        "param_dict = {\"criterion\" :[\"gini\", \"entropy\"],\n",
        "            \"max_depth\" :range(1,8),\n",
        "            #\"min_samples_split\" :range(2,10),\n",
        "            #\"min_samples_leaf\" :range(1,5),\n",
        "             \"ccp_alpha\" :[0.0001, 0.001, 0.01, 0.1]}\n",
        "\n",
        "#tree_class = DecisionTreeClassifier(random_state=1024)\n",
        "tree_class = DecisionTreeClassifier()\n",
        "\n",
        "grid = GridSearchCV(estimator=tree_class,\n",
        "        param_grid = param_dict,\n",
        "        cv = 10,\n",
        "        verbose= 1,\n",
        "        n_jobs = 1)\n",
        "\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nISY_-Q5DUEz"
      },
      "outputs": [],
      "source": [
        "grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJz9jLmnIeDc"
      },
      "outputs": [],
      "source": [
        "# Update model\n",
        "\n",
        "model_dtc_p=DecisionTreeClassifier(ccp_alpha = 0.0001,\n",
        "                                   criterion ='gini',\n",
        "                                    max_depth = 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED1bLZ9eIgf7"
      },
      "outputs": [],
      "source": [
        "#Fit and score the model\n",
        "\n",
        "model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "score_train = model_dtc_p.score(X_train, y_train)\n",
        "print(\"score_train: \", score_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjYJOV91IokF"
      },
      "outputs": [],
      "source": [
        "# Score the test dataset\n",
        "\n",
        "score_test = model_dtc_p.score(X_test, y_test)\n",
        "print(\"score_test: \", score_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRmo5Z7FIq2J"
      },
      "outputs": [],
      "source": [
        "# ROC curve and AUC\n",
        "\n",
        "# generate a no skill prediction (majority class)\n",
        "ns_probs = [0 for _ in range(len(y_test))]\n",
        "\n",
        "# fit a model\n",
        "#model = DecisionTreeClassifier()\n",
        "model_dtc_p.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# predict probabilities\n",
        "lr_probs = model_dtc_p.predict_proba(X_test)\n",
        "\n",
        "# keep probabilities for the positive outcome only\n",
        "lr_probs = lr_probs[:, 1]\n",
        "\n",
        "# calculate scores\n",
        "ns_auc = roc_auc_score(y_test, ns_probs)\n",
        "lr_auc = roc_auc_score(y_test, lr_probs)\n",
        "\n",
        "# summarize scores\n",
        "print('Churn: ROC AUC=%.3f' % (ns_auc))\n",
        "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
        "\n",
        "# calculate roc curves\n",
        "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Churn')\n",
        "plt.plot(lr_fpr, lr_tpr, marker='X', label='Prediction')\n",
        "\n",
        "# axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "\n",
        "# show the legend\n",
        "plt.legend()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E02xj1O5I1B_"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, model_dtc.predict(X_test))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.imshow(cm)\n",
        "ax.grid(False)\n",
        "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
        "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
        "ax.set_ylim(1.5, -0.5)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuKmYDqLI57g"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_pred = model_dtc_p.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8SgkwIQl82K"
      },
      "source": [
        "Re-check for overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HTDLaLWl_gu"
      },
      "outputs": [],
      "source": [
        "# evaluate decision tree performance on train and test sets with different tree depths\n",
        "# define lists to collect scores\n",
        "train_scores, test_scores = list(), list()\n",
        "# define the tree depths to evaluate\n",
        "values = [i for i in range(1, 9)]\n",
        "# evaluate a decision tree for each depth\n",
        "for i in values:\n",
        " # configure the model\n",
        " model_dtc_p = DecisionTreeClassifier(max_depth=i)\n",
        " # fit model on the training dataset\n",
        " model_dtc_p.fit(X_train, y_train)\n",
        " # evaluate on the train dataset\n",
        " train_yhat = model_dtc_p.predict(X_train)\n",
        " train_acc = accuracy_score(y_train, train_yhat)\n",
        " train_scores.append(train_acc)\n",
        " # evaluate on the test dataset\n",
        " test_yhat = model_dtc_p.predict(X_test)\n",
        " test_acc = accuracy_score(y_test, test_yhat)\n",
        " test_scores.append(test_acc)\n",
        " # summarize progress\n",
        " print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
        "# plot of train and test scores vs tree depth\n",
        "plt.plot(values, train_scores, '-o', label='Train')\n",
        "plt.plot(values, test_scores, '-o', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2og7GM65z1zg"
      },
      "source": [
        "Much better. But does it hold on a fresh dataset ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPaYlFmn1SUo"
      },
      "source": [
        "Load new data.\n",
        "Need to make sure both datasets have exactly the same feature in the SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcXRQQB21D1j"
      },
      "outputs": [],
      "source": [
        "df_raw_may = pd.read_csv(r'/content/qp_model_052023_202301071833.csv')\n",
        "#print(df_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRn5Jz1K3n6u"
      },
      "outputs": [],
      "source": [
        "df_raw_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH6jVJBh3n6v"
      },
      "outputs": [],
      "source": [
        "df_raw_may.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDJ36VnO3n6v"
      },
      "outputs": [],
      "source": [
        "df_raw_may.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5K9zZ5K3n6v"
      },
      "outputs": [],
      "source": [
        "df_raw_may.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqc2j4aq3n6w"
      },
      "outputs": [],
      "source": [
        "# Look for Null values\n",
        "df_raw_may.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npqjk7SW3n6w"
      },
      "outputs": [],
      "source": [
        "nan_count = df_raw_may.isna().sum()\n",
        "print(nan_count )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHDAJyy_3n6w"
      },
      "outputs": [],
      "source": [
        "df_raw_may = df_raw_may.drop (['total_points','count_of_approved_accounts_hist','master','state',\n",
        "                       'groupchannel','eligibilityprogram','devicebrand','bqplancode','is_ported'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VSsPNw_3n6w"
      },
      "outputs": [],
      "source": [
        "df_raw_may.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9dEGZRz3n6w"
      },
      "outputs": [],
      "source": [
        "df_raw_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2N6t5QN6KDy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7uILrSs6KGE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgH8_7dy6KIK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbhMp4-I6KK3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_pdkHF73n6x"
      },
      "outputs": [],
      "source": [
        "# age is a continuous variable that will need to be normalized.\n",
        "\n",
        "column = 'age'\n",
        "\n",
        "df_raw_may[column] = df_raw_may[column] /df_raw_may[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3voFVW_Z3n6x"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'data_points'\n",
        "\n",
        "df_raw_may[column] = df_raw_may[column] /df_raw_may[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2taKs083n6x"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'voice_points'\n",
        "\n",
        "df_raw_may[column] = df_raw_may[column] /df_raw_may[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PUaBGac3n6x"
      },
      "outputs": [],
      "source": [
        "# data fields are continuous variables that will need to be normalized.\n",
        "\n",
        "column = 'sms_points'\n",
        "\n",
        "df_raw_may[column] = df_raw_may[column] /df_raw_may[column].abs().max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DUE1GJi3n6x"
      },
      "outputs": [],
      "source": [
        "# Flatten the categorical features (where datatype = Object)\n",
        "\n",
        "df_raw_may = pd.get_dummies(df_raw_may, columns=['plantype'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiex2B6L6KBb"
      },
      "outputs": [],
      "source": [
        "# Oversample the minority class. This needs to be done after flattening the df to maintain oversampled values.\n",
        "\n",
        "count_class_0, count_class_1 = df_raw_may.churn_flag.value_counts()\n",
        "# Divide by class\n",
        "df_class_0_may = df_raw_may[df_raw_may['churn_flag'] == 0] #majority class\n",
        "df_class_1_may = df_raw_may[df_raw_may['churn_flag'] == 1] #minority class\n",
        "\n",
        "df_class_1_over_may = df_class_1_may.sample(count_class_0, replace=True)\n",
        "df_over_may = pd.concat([df_class_0_may, df_class_1_over_may], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_over_may.churn_flag.value_counts())\n",
        "\n",
        "df_over_may.churn_flag.value_counts().plot(kind='bar', title='Count (target)')\n",
        "\n",
        "plt.style.use('bmh')\n",
        "plt.figure(figsize=(20, 16))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNP2ZK065DCk"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing dataset.\n",
        "\n",
        "train_data_may, test_data_may = train_test_split(df_over_may,train_size=0.1)\n",
        "display(train_data_may)\n",
        "display(test_data_may)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ixuZ8dk5DCx"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "X_train_may = train_data_may.iloc[:,1:]\n",
        "X_train_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHyIJmqM5DCx"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the training data\n",
        "\n",
        "y_train_may = train_data_may.iloc[:, [0]]\n",
        "y_train_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQIn_9wy5DCx"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "X_test_may = test_data_may.iloc[:,1:]\n",
        "X_test_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqAmUOKI5DCx"
      },
      "outputs": [],
      "source": [
        "# Split the target from the feature for the test data\n",
        "\n",
        "y_test_may = test_data_may.iloc[:, [0]]\n",
        "y_test_may.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxsA-KnZ5DCx"
      },
      "outputs": [],
      "source": [
        "#Create Decision Tree classifer object\n",
        "\n",
        "model_dtc = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9T4UsS_5DCx"
      },
      "outputs": [],
      "source": [
        "# Fit and score the model\n",
        "\n",
        "#model_dtc.fit(X_train, y_train.values.ravel())\n",
        "#score_train = model_dtc.score(X_train, y_train)\n",
        "#print(\"score_train: \", score_train)\n",
        "#print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONUWX0wX5DCy"
      },
      "outputs": [],
      "source": [
        "# Score the test dataset\n",
        "model_dtc.fit(X_test_may, y_test_may.values.ravel())\n",
        "score_test = model_dtc.score(X_test_may, y_test_may)\n",
        "print(\"score_test: \", score_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW5Ig9uQ5DCy"
      },
      "outputs": [],
      "source": [
        "y_pred_may = model_dtc.predict(X_test_may)\n",
        "cm = confusion_matrix(y_test_may, y_pred_may)\n",
        "cr = classification_report(y_test_may, y_pred_may)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "008PAfatJAFA"
      },
      "outputs": [],
      "source": [
        "# Visualization of SHaP\n",
        "# https://towardsdatascience.com/demystify-your-ml-model-with-shap-fc191a1cb08a\n",
        "\n",
        "model_dtc_p = model_dtc_p.fit(X_train, y_train)\n",
        "\n",
        "exp = TreeExplainer(model_dtc_p)\n",
        "sv = exp.shap_values(X_train)\n",
        "summary_plot(sv[1], X_train, max_display=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEY4HH6S0gly"
      },
      "source": [
        "It's interesting that while directionaly consistent, ACL+LL differs in impact values from LL+ACP. Perhaps this is an artifact from the stochastic nature of the technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4l_H0IphUAZ"
      },
      "source": [
        "<h2>Recursive Feature Elimination (RFE)</h2><br>\n",
        "https://machinelearningmastery.com/rfe-feature-selection-in-python/<br>\n",
        "https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCyxneJVkK6R"
      },
      "source": [
        "The DTC model above reached the following with 6 dimensions<br>\n",
        "SMS, Data, Voice, Age, Eligibility, plantype <br><br>\n",
        "\n",
        "Input deck: qp_model_202301062016.csv<br>\n",
        "HP Tuning: None<br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>Class</td>\n",
        "    <td>Recall</td>\n",
        "    <td>Precision</td>\n",
        "    <td>f1-score</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>0.92</td>\n",
        "    <td>0.83</td>\n",
        "    <td>0.87</td>\n",
        "  </tr>\n",
        "   <tr>\n",
        "    <td>0</td>\n",
        "    <td>0.81</td>\n",
        "    <td>0.92</td>\n",
        "    <td>0.86</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSIcWmT1BYWf"
      },
      "outputs": [],
      "source": [
        "# evaluate RFE for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "# create pipeline\n",
        "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
        "model = DecisionTreeClassifier()\n",
        "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From https://www.activestate.com/blog/top-10-tools-for-hyperparameter-optimization-in-python/\n",
        "The Bayesian-Optimization Library\n",
        "Scikit-Optimize\n",
        "GPyOpt\n",
        "Hyperopt\n",
        "SHERPA\n",
        "Optuna\n",
        "Ray Tune\n",
        "Neural Network Intelligence (NNI)\n",
        "MLMachine\n",
        "Talos\n",
        "GridSearchCV\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YplVQg4fq1m5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14eeOjLjhLmVNTrPOGXUvFPbjkXKAlfJe",
      "authorship_tag": "ABX9TyPSSdMKKNAlw8tcRa+mi8FR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}